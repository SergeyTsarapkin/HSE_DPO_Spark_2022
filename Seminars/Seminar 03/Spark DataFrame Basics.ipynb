{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 03: Основы Spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Находнов Максим (nakhodnov17@gmail.com)`\n",
    "#### `Москва, 2023`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* DataFrame и SQL API\n",
    "* Базовые операции в Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxEu0kipR2jE"
   },
   "source": [
    "### `Монтируем диск для хранения данных`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:02.765330Z",
     "start_time": "2023-05-03T03:55:02.755555Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2wc9zsKFgAu",
    "outputId": "fa43c81e-d5ce-4911-cd10-e823b3a1220a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:07.761968Z",
     "start_time": "2023-05-03T03:55:03.004378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTs7dYpxO6pF",
    "outputId": "0bd5516e-afb7-40b0-a0b9-fd0ead01470e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -q pyspark pyarrow kaggle parquet-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:16.396171Z",
     "start_time": "2023-05-03T03:55:07.766558Z"
    },
    "id": "JTjLPr5jFhpR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 06:55:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Создаём конфигурационный класс с параметрами подключения к серверу\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        # Указываем порт на котором будет располагаться UI\n",
    "        .set('spark.ui.port', '4050')\n",
    "        # Указываем URL master ноды Spark кластера\n",
    "        # Можно использовать local mode, указав `local[<number_cores>]`\n",
    "        # В таком случае вся обработка будет происходить на текущем компьютере\n",
    "        # При этом, это может давать преимущество ввиду наличия параллелизма по ядрам компьютера\n",
    "        .setMaster('local[*]')\n",
    "        # Если нужно подключиться к \"реальному\" кластеру то нужно указать URL `spark://<master-node-url:master-node-url>`. Например:\n",
    "        # .setMaster('spark://localhost:7077')\n",
    ")\n",
    "# Создаём точку доступа на кластер. Позволят использовать RDD API\n",
    "sc = SparkContext(conf=conf)\n",
    "# Точка доступа для использования DataFrame API\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# По завершении программы нужно обязательно выполнить остановку подключения для освобождения занятых ресурсов\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEuclFqwR5e0"
   },
   "source": [
    "### `Магия для просмотра Spark UI`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе в Google Colab/Kaggle можно использовать прокси для доступа к веб-интерфейсу Spark. Для установки и запуска прокси выполните команды ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "aGj6Pkr5Owrh",
    "outputId": "218f4c49-42af-4c9b-82eb-a9f38f437c67",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-28 10:00:06--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 54.161.241.46, 54.237.133.81, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13832437 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.19M  53.8MB/s    in 0.2s    \n",
      "\n",
      "2023-01-28 10:00:07 (53.8 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n"
     ]
    }
   ],
   "source": [
    "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "! unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jJo9TTf3Psw4",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok authtoken <token from https://dashboard.ngrok.com/get-started/your-authtoken>')\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "mwMH1sFcPXCH",
    "outputId": "0d9a49fd-f005-471c-f29b-114309a022dd",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://baba-34-74-244-185.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SOyISvrSIun"
   },
   "source": [
    "### `Загрузка данных`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMohSEhKSN0R"
   },
   "source": [
    "Предварительно нужно скачать `kaggle.json` из [настроек аккаунта Kaggle](https://www.kaggle.com/settings). Положите его в папку `~./.kaggle`. На Linux/MacOS это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:18.912291Z",
     "start_time": "2023-05-03T03:55:16.399069Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLhF993mGCQx",
    "outputId": "1b9ae695-e4f1-4eaa-f03e-0fba9ae93c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/nakhodnov/.kaggle/: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir ~/.kaggle/\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMohSEhKSN0R"
   },
   "source": [
    "В этом проекте нужно работать с данными для предсказания спроса: [M5 Forecasting](https://www.kaggle.com/competitions/m5-forecasting-accuracy/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:18.921412Z",
     "start_time": "2023-05-03T03:55:18.917244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Используйте путь на Google Drive при работе в Google Colab\n",
    "path = '/content/drive/MyDrive/m5-forecasting-accuracy'\n",
    "# Или локальный путь\n",
    "path = './m5-forecasting-accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:24.526445Z",
     "start_time": "2023-05-03T03:55:18.925305Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLhF993mGCQx",
    "outputId": "1b9ae695-e4f1-4eaa-f03e-0fba9ae93c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading m5-forecasting-accuracy.zip to /Users/nakhodnov/HSE_DPO_Spark_2022/Seminars/Seminar 03\n",
      " 90%|██████████████████████████████████    | 41.0M/45.8M [00:02<00:00, 28.1MB/s]\n",
      "100%|██████████████████████████████████████| 45.8M/45.8M [00:02<00:00, 22.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle competitions download -c m5-forecasting-accuracy\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('./m5-forecasting-accuracy.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:25.408666Z",
     "start_time": "2023-05-03T03:55:24.529366Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jvOMphiFwRi",
    "outputId": "ab014389-903e-4e9d-8967-ba3d203334ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar.csv                sample_submission.csv\r\n",
      "sales_train_evaluation.csv  sell_prices.csv\r\n",
      "sales_train_validation.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:42.286781Z",
     "start_time": "2023-05-03T03:55:25.412229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "0aoAjM9VGUQC",
    "outputId": "5bb02e2e-ea31-4470-f2b0-290754056740"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 06:55:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "5  HOBBIES_1_006_CA_1_validation  HOBBIES_1_006  HOBBIES_1  HOBBIES     CA_1   \n",
       "6  HOBBIES_1_007_CA_1_validation  HOBBIES_1_007  HOBBIES_1  HOBBIES     CA_1   \n",
       "7  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "8  HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "9  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "5       CA    0    0    0    0  ...       0       1       0       1       0   \n",
       "6       CA    0    0    0    0  ...       0       0       0       1       0   \n",
       "7       CA   12   15    0    0  ...       0       0       1      37       3   \n",
       "8       CA    2    0    7    3  ...       0       0       1       1       6   \n",
       "9       CA    0    0    1    0  ...       1       0       0       0       0   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "5       0       0       2       0       0  \n",
       "6       1       0       0       1       1  \n",
       "7       4       6       3       2       1  \n",
       "8       0       0       0       0       0  \n",
       "9       0       0       2       0       2  \n",
       "\n",
       "[10 rows x 1919 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Зададим пути к файлам из датасета\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "file_validation = f\"{path}/sales_train_validation.csv\"\n",
    "file_evaluation = f\"{path}/sales_train_evaluation.csv\"\n",
    "file_prices = f\"{path}/sell_prices.csv\"\n",
    "\n",
    "# Формат данных — CSV\n",
    "file_type = \"csv\"\n",
    "# Зададим параметры, как интерпретировать загруженные данные\n",
    "# Определять типы колонок автоматически\n",
    "infer_schema = \"true\"\n",
    "# Интерпретируем первую строку в файле, как названия колонок\n",
    "first_row_is_header = \"true\"\n",
    "# Задаём разделитель между значениями колонок\n",
    "delimiter = \",\"\n",
    "\n",
    "df_validation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_validation)\n",
    "    # Также, можно указывать пути в hdfs или базы данных, например, Hive\n",
    "#       .load('hdfs:///path_to_data/...')\n",
    ")\n",
    "\n",
    "df_evaluation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_evaluation)\n",
    ")\n",
    "df_prices = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_prices)\n",
    ")\n",
    "\n",
    "# Возьмём первые 10 строк pyspark.sql.dataframe.DataFrame\n",
    "# И выполним action для преобразования в pandas.DataFrame\n",
    "df_validation.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCsO86XSW9kW"
   },
   "source": [
    "### `Spark DataFrame API`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fw_vMfqYd2b"
   },
   "source": [
    "* [Quickstart](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)\n",
    "* [Документация](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:42.365772Z",
     "start_time": "2023-05-03T03:55:42.289608Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCbwFDIGXEcA",
    "outputId": "c27b98b7-f064-4897-e2cb-57ebc1f832a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: bigint, name: string, dept_id: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_data = [\n",
    "    (1, 'Smith', 10),\n",
    "    (2, 'Rose', 20),\n",
    "    (3, 'Williams', 10),\n",
    "    (4, 'Jones', 30),\n",
    "    (5, 'Jones', None),\n",
    "]\n",
    "emp_columns = ['emp_id', 'name', 'dept_id']\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
    "emp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:42.373636Z",
     "start_time": "2023-05-03T03:55:42.368591Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOdJOWisX_Aj",
    "outputId": "c6dd0501-3a49-4f07-fcf4-db392553ed6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод DataFrame не показывает его содержимое, так как оно ещё не было вычислено, так как вычисления в Spark происходят только в момент вызова action.\n",
    "\n",
    "Примеры action:\n",
    "* `count()` — подсчитывает число строк в DataFrame\n",
    "* `toPandas()` — преобразует Spark DataFrame в pandas DataFrame\n",
    "* `collect()` — выполняет вычисление текущего Spark DataFrame и возвращает результат\n",
    "* `show()` — `collect()` + pretty print результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:43.700269Z",
     "start_time": "2023-05-03T03:55:42.381922Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|emp_id|    name|dept_id|\n",
      "+------+--------+-------+\n",
      "|     1|   Smith|     10|\n",
      "|     2|    Rose|     20|\n",
      "|     3|Williams|     10|\n",
      "|     4|   Jones|     30|\n",
      "|     5|   Jones|   null|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая информация о данных — названия колонок и их типы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:43.708843Z",
     "start_time": "2023-05-03T03:55:43.703778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['emp_id', 'name', 'dept_id'],\n",
       " StructType([StructField('emp_id', LongType(), True), StructField('name', StringType(), True), StructField('dept_id', LongType(), True)]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.columns, emp_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие методы дублируются по аналогии с `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.283929Z",
     "start_time": "2023-05-03T03:55:43.712912Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhTSzDDX5zWw",
    "outputId": "8cd5a932-fefe-4b1c-ce40-615a057e1269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|emp_id|    name|dept_id|\n",
      "+------+--------+-------+\n",
      "|     1|   Smith|     10|\n",
      "|     2|    Rose|     20|\n",
      "|     3|Williams|     10|\n",
      "|     4|   Jones|     30|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame состоит из колонок. Получение колонки возможно через атрибуты или через индексацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.300002Z",
     "start_time": "2023-05-03T03:55:44.287375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'name'>, Column<'name'>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.name, emp_df['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо самих значений колонок, в соответствии с принципом \"ленивых\" вычислений, возвращаются ссылки на них. Такие колонки могут участвовать в символьных вычислениях. Например, к ним можно применять арифметические, булевы операции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.320080Z",
     "start_time": "2023-05-03T03:55:44.303113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(((dept_id - 20) / 10) > emp_id)'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_expr = (emp_df.dept_id - 20) / 10 > emp_df.emp_id\n",
    "column_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные **колоночные выражения** (**column expression**) можно вычислять:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.805817Z",
     "start_time": "2023-05-03T03:55:44.323998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|(((dept_id - 20) / 10) > emp_id)|\n",
      "+--------------------------------+\n",
      "|                           false|\n",
      "|                           false|\n",
      "|                           false|\n",
      "|                           false|\n",
      "|                            null|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(column_expr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонку можно переименовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:45.309115Z",
     "start_time": "2023-05-03T03:55:44.810516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dept_id squared|\n",
      "+---------------+\n",
      "|          100.0|\n",
      "|          400.0|\n",
      "|          100.0|\n",
      "|          900.0|\n",
      "|           null|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select((emp_df.dept_id ** 2).alias('dept_id squared')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для DataFrame доступны SQL подобные операции, например, `join`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:45.758201Z",
     "start_time": "2023-05-03T03:55:45.311701Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpatCy-5XsvD",
    "outputId": "37be9337-77ec-41ee-9cd0-4c298fe6e503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [\n",
    "    ('Finance', 10),\n",
    "    ('Marketing', 20),\n",
    "    ('Sales', 30),\n",
    "    ('IT', 40),\n",
    "]\n",
    "dept_columns = ['dept_name', 'dept_id']\n",
    "\n",
    "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:46.982880Z",
     "start_time": "2023-05-03T03:55:45.760896Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhU1JequYEvz",
    "outputId": "d975a439-0c5a-41ce-83c5-ed31e8c64bca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+\n",
      "|dept_id|emp_id|    name|dept_name|\n",
      "+-------+------+--------+---------+\n",
      "|     10|     1|   Smith|  Finance|\n",
      "|     10|     3|Williams|  Finance|\n",
      "|     20|     2|    Rose|Marketing|\n",
      "|     30|     4|   Jones|    Sales|\n",
      "+-------+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, how='inner', on=['dept_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:47.717944Z",
     "start_time": "2023-05-03T03:55:46.989769Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AmpYXEEZVcg",
    "outputId": "a4c37de4-5b37-4c69-ad7c-accecf6892cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+\n",
      "|dept_id|emp_id|    name|dept_name|\n",
      "+-------+------+--------+---------+\n",
      "|   null|     5|   Jones|     null|\n",
      "|     10|     3|Williams|  Finance|\n",
      "|     10|     1|   Smith|  Finance|\n",
      "|     20|     2|    Rose|Marketing|\n",
      "|     30|     4|   Jones|    Sales|\n",
      "|     40|  null|    null|       IT|\n",
      "+-------+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:===================================================>    (11 + 1) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, how='outer', on=['dept_id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, доступна фильтрация и сортировка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:48.557959Z",
     "start_time": "2023-05-03T03:55:47.723042Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddeQGejsaFfV",
    "outputId": "73b5fa01-5fcd-42f7-9ed9-ee94c774b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+---------+\n",
      "|dept_id|emp_id| name|dept_name|\n",
      "+-------+------+-----+---------+\n",
      "|     10|     1|Smith|  Finance|\n",
      "|     20|     2| Rose|Marketing|\n",
      "+-------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    emp_df\n",
    "      .join(dept_df, how='outer', on=['dept_id'])\n",
    "      # Обратите внимание на колоночное выражение в фильтре\n",
    "      .where((emp_df['name'] == 'Smith') | (emp_df['name'] == 'Rose'))\n",
    "      .sort('dept_id')\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с колонками обычно выполняется через колоночные выражения. Их можно использовать, например, для выполнения join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:49.022712Z",
     "start_time": "2023-05-03T03:55:48.562014Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfHsc6tvale3",
    "outputId": "2d26ce94-5c4c-49d8-f4a2-e520524d5a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|emp_id|    name|emp_dept_id|\n",
      "+------+--------+-----------+\n",
      "|     1|   Smith|         10|\n",
      "|     2|    Rose|         20|\n",
      "|     3|Williams|         10|\n",
      "|     4|   Jones|         30|\n",
      "|     5|   Jones|       null|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_columns_renamed = ['emp_id', 'name', 'emp_dept_id']\n",
    "\n",
    "emp_renamed_df = spark.createDataFrame(emp_data, emp_columns_renamed)\n",
    "emp_renamed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:49.679335Z",
     "start_time": "2023-05-03T03:55:49.025389Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mM2ieylzbWlj",
    "outputId": "2547f5c2-77f7-4618-e3a4-fe84005d2fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|\n",
      "+------+--------+-----------+---------+-------+\n",
      "|     1|   Smith|         10|  Finance|     10|\n",
      "|     3|Williams|         10|  Finance|     10|\n",
      "|     2|    Rose|         20|Marketing|     20|\n",
      "|     4|   Jones|         30|    Sales|     30|\n",
      "+------+--------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_renamed_df.join(\n",
    "    dept_df, emp_renamed_df.emp_dept_id == dept_df.dept_id,  how='inner'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименование колонок также возможно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:50.343395Z",
     "start_time": "2023-05-03T03:55:49.682852Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDhzSDSsbhY3",
    "outputId": "c8c69547-d309-44c7-ce74-6cb8d017589b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+\n",
      "|dept_id|emp_id|    name|dept_name|\n",
      "+-------+------+--------+---------+\n",
      "|     10|     1|   Smith|  Finance|\n",
      "|     10|     3|Williams|  Finance|\n",
      "|     20|     2|    Rose|Marketing|\n",
      "|     30|     4|   Jones|    Sales|\n",
      "+-------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    emp_renamed_df\n",
    "      .withColumnRenamed('emp_dept_id', 'dept_id')\n",
    "      .join(\n",
    "          dept_df, 'dept_id',  how='inner'\n",
    "      )\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для преобразования колонок в модуле `pyspark.sql.functions` содержится большой набор вспомогательных функций. Например:\n",
    "* Вспомогательные: `lit`, `col`, ...\n",
    "* Поэлементные математические функции: `cos`, `sin`, `round`, ...\n",
    "* Поэлементные функции для работы датами и временем: `dayofmonth`, ...\n",
    "* Агрегаторы: `sum`, `mean`, ...\n",
    "* Функции для работы с коллекциями (сложными данными, хранящимся в колонке): `array_sort`, `concat`, ...\n",
    "* Сортировки: `asc`, ...\n",
    "* Строковые функции: `concat_ws`, `lower`, `split`, ...\n",
    "* Оконные функции: `lag`, ...\n",
    "* Преобразования с пользовательскими функциями: `udf_pandas`, ...\n",
    "\n",
    "**Всегда перед написанием кода нужно подумать, нет ли уже готовой функции в данном модуле. Использование готовых функции существенно влияет на скорость вычислений.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:50.351376Z",
     "start_time": "2023-05-03T03:55:50.347205Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для применения функций нужно поменять тип колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:50.976103Z",
     "start_time": "2023-05-03T03:55:50.354861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+----------+\n",
      "|emp_id|    name|dept_id| hire_date|\n",
      "+------+--------+-------+----------+\n",
      "|     1|   Smith|     10|2000-01-10|\n",
      "|     2|    Rose|     20|2010-02-20|\n",
      "|     3|Williams|     10|2000-03-10|\n",
      "|     4|   Jones|     30|2020-04-30|\n",
      "+------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_with_date = (\n",
    "    emp_df\n",
    "        .dropna()\n",
    "        .withColumn(\n",
    "            'hire_date', \n",
    "            # Конструируем дату в формате yyyy-mm-dd \n",
    "            F.concat_ws(\n",
    "                '-', \n",
    "                # Придумываем год\n",
    "                (1990 + emp_df.dept_id).cast(StringType()),\n",
    "                # Придумываем месяц\n",
    "                F.concat(F.lit('0'), emp_df.emp_id.cast(StringType())), \n",
    "                # Придумываем день\n",
    "                emp_df.dept_id.cast(StringType())\n",
    "            ).cast(DateType())\n",
    "        )\n",
    ")\n",
    "emp_with_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:51.440025Z",
     "start_time": "2023-05-03T03:55:50.979677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------+\n",
      "|ACOS((emp_id / 4))|year(hire_date)|processed_name|\n",
      "+------------------+---------------+--------------+\n",
      "| 1.318116071652818|           2000|          cмит|\n",
      "|1.0471975511965979|           2010|          rose|\n",
      "|0.7227342478134157|           2000|      williams|\n",
      "|               0.0|           2020|         jones|\n",
      "+------------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_with_date.select(\n",
    "    F.acos(emp_with_date.emp_id / 4),\n",
    "    F.year(emp_with_date.hire_date),\n",
    "    F.regexp_replace(F.lower(emp_with_date.name), 'smith', 'cмит').alias('processed_name')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO38c_qmcYCT"
   },
   "source": [
    "### `Spark SQL API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:51.857953Z",
     "start_time": "2023-05-03T03:55:51.443880Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdpoa0q-crM4",
    "outputId": "af4c7c0a-5bc4-4c60-d954-c9e3d89b964d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------+-----+\n",
      "|emp_id|        address|    city|state|\n",
      "+------+---------------+--------+-----+\n",
      "|     1|   1523 Main St|     SFO|   CA|\n",
      "|     2| 3453 Orange St|     SFO|   NY|\n",
      "|     3|   34 Warner St|  Jersey|   NJ|\n",
      "|     4|221 Cavalier St|  Newark|   DE|\n",
      "|     5|  789 Walnut St|Sandiago|   CA|\n",
      "+------+---------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_data = [\n",
    "    (1, '1523 Main St', 'SFO', 'CA'),\n",
    "    (2, '3453 Orange St', 'SFO', 'NY'),\n",
    "    (3, '34 Warner St', 'Jersey', 'NJ'),\n",
    "    (4, '221 Cavalier St', 'Newark', 'DE'),\n",
    "    (5, '789 Walnut St', 'Sandiago', 'CA')\n",
    "]\n",
    "add_columns = ['emp_id', 'address', 'city', 'state']\n",
    "\n",
    "add_df = spark.createDataFrame(add_data, add_columns)\n",
    "add_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark позволяет использовать DataFrame в качестве таблиц в регулярных SQL запросах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:51.938821Z",
     "start_time": "2023-05-03T03:55:51.869856Z"
    },
    "id": "8QoIUY3mcLOm"
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView('EMP')\n",
    "dept_df.createOrReplaceTempView('DEPT')\n",
    "add_df.createOrReplaceTempView('ADD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:53.075199Z",
     "start_time": "2023-05-03T03:55:51.941888Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evVxaSO9dRvJ",
    "outputId": "598170d5-2b2a-4d79-ceee-1b74fdd252a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+---------+-------+------+---------------+------+-----+\n",
      "|emp_id|    name|dept_id|dept_name|dept_id|emp_id|        address|  city|state|\n",
      "+------+--------+-------+---------+-------+------+---------------+------+-----+\n",
      "|     1|   Smith|     10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     2|    Rose|     20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     3|Williams|     10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     4|   Jones|     30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-------+---------+-------+------+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select * from EMP e, DEPT d, ADD a\n",
    "    where e.dept_id == d.dept_id and e.emp_id == a.emp_id\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPKx2_0eJAG"
   },
   "source": [
    "### `Ещё базовые операции над Spark DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:53.520078Z",
     "start_time": "2023-05-03T03:55:53.078716Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3RTYC2LdsR7",
    "outputId": "8e8d9960-217c-4837-b77b-f8996e526521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   Name|      Dept|Salary|\n",
      "+-------+----------+------+\n",
      "|  James|     Sales|  3000|\n",
      "|Michael|     Sales|  4600|\n",
      "| Robert|     Sales|  4100|\n",
      "|  Maria|   Finance|  3000|\n",
      "|  James|     Sales|  3000|\n",
      "|  Scott|   Finance|  3300|\n",
      "|    Jen|   Finance|  3900|\n",
      "|   Jeff| Marketing|  3000|\n",
      "|  Kumar| Marketing|  2000|\n",
      "|   Saif|     Sales|  4100|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Michael', 'Sales', 4600),\n",
    "    ('Robert', 'Sales', 4100),\n",
    "    ('Maria', 'Finance', 3000),\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Scott', 'Finance', 3300),\n",
    "    ('Jen', 'Finance', 3900),\n",
    "    ('Jeff', ' Marketing', 3000),\n",
    "    ('Kumar', 'Marketing', 2000),\n",
    "    ('Saif', 'Sales', 4100),\n",
    "]\n",
    "columns = ['Name', 'Dept', 'Salary']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:53.979903Z",
     "start_time": "2023-05-03T03:55:53.523202Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYR2TObjerC1",
    "outputId": "3bbcd476-9e47-40ba-d9e0-7121978e41e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   Name|      Dept|Salary|\n",
      "+-------+----------+------+\n",
      "|  James|     Sales|  3000|\n",
      "|Michael|     Sales|  4600|\n",
      "| Robert|     Sales|  4100|\n",
      "|  Maria|   Finance|  3000|\n",
      "|  Scott|   Finance|  3300|\n",
      "|    Jen|   Finance|  3900|\n",
      "|   Jeff| Marketing|  3000|\n",
      "|  Kumar| Marketing|  2000|\n",
      "|   Saif|     Sales|  4100|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:54.411672Z",
     "start_time": "2023-05-03T03:55:53.982570Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyZ0mIvsethd",
    "outputId": "d890ce5d-ca5e-49d1-b7b1-73d0c6a964ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, возможно использовать группировку и агрегаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:54.932782Z",
     "start_time": "2023-05-03T03:55:54.415371Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACpgoLAQe1wq",
    "outputId": "c36996e9-b54a-49b9-d875-1ad46969f677"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Dept='Sales', sum(Salary)=18800),\n",
       " Row(Dept='Finance', sum(Salary)=10200),\n",
       " Row(Dept=' Marketing', sum(Salary)=3000),\n",
       " Row(Dept='Marketing', sum(Salary)=2000)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('Dept').sum().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtBLc8D6hJGb"
   },
   "source": [
    "### `IO операции`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:54.969927Z",
     "start_time": "2023-05-03T03:55:54.936851Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnoAthLTgWhk",
    "outputId": "818089ee-fd43-4e3d-d8c2-00eb63e80c70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[min_salary: bigint, mean_salary: double, max_salary: bigint]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_statistics = df.select(\n",
    "    F.min('Salary').alias('min_salary'),\n",
    "    F.mean('Salary').alias('mean_salary'),\n",
    "    F.max('Salary').alias('max_salary')\n",
    ")\n",
    "# Пока никаких вычислений не произошло\n",
    "base_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:56.864684Z",
     "start_time": "2023-05-03T03:55:54.975212Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "pb8lH5i0iZ8g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 87:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_statistics.write.csv('./base_statistics.csv', header=True)\n",
    "base_statistics.write.parquet('./base_statistics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:56:10.314253Z",
     "start_time": "2023-05-03T03:56:09.473927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-5f1b82c2-0fe0-4305-8c71-067297fd29b2-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls ./base_statistics.csv/\n",
    "%pycat ./base_statistics.csv/part-00000-5f1b82c2-0fe0-4305-8c71-067297fd29b2-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:56:25.040198Z",
     "start_time": "2023-05-03T03:56:22.429642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-69467b95-0711-4c15-a58c-e6370e0886c9-c000.snappy.parquet\n",
      "\n",
      "############ file meta data ############\n",
      "created_by: parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\n",
      "num_columns: 3\n",
      "num_rows: 1\n",
      "num_row_groups: 1\n",
      "format_version: 1.0\n",
      "serialized_size: 789\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "############ Columns ############\n",
      "min_salary\n",
      "mean_salary\n",
      "max_salary\n",
      "\n",
      "############ Column(min_salary) ############\n",
      "name: min_salary\n",
      "path: min_salary\n",
      "max_definition_level: 1\n",
      "max_repetition_level: 0\n",
      "physical_type: INT64\n",
      "logical_type: None\n",
      "converted_type (legacy): NONE\n",
      "compression: SNAPPY (space_saved: -5%)\n",
      "\n",
      "############ Column(mean_salary) ############\n",
      "name: mean_salary\n",
      "path: mean_salary\n",
      "max_definition_level: 1\n",
      "max_repetition_level: 0\n",
      "physical_type: DOUBLE\n",
      "logical_type: None\n",
      "converted_type (legacy): NONE\n",
      "compression: SNAPPY (space_saved: -5%)\n",
      "\n",
      "############ Column(max_salary) ############\n",
      "name: max_salary\n",
      "path: max_salary\n",
      "max_definition_level: 1\n",
      "max_repetition_level: 0\n",
      "physical_type: INT64\n",
      "logical_type: None\n",
      "converted_type (legacy): NONE\n",
      "compression: SNAPPY (space_saved: -5%)\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "%ls ./base_statistics.parquet/\n",
    "! parquet-tools inspect base_statistics.parquet/part-00000-69467b95-0711-4c15-a58c-e6370e0886c9-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:56:27.179866Z",
     "start_time": "2023-05-03T03:56:26.910581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|min_salary|mean_salary|max_salary|\n",
      "+----------+-----------+----------+\n",
      "|      2000|     3400.0|      4600|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataFrame[min_salary: int, mean_salary: double, max_salary: int], None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df = (\n",
    "    spark.read\n",
    "        .format('csv')\n",
    "        .option(\"inferSchema\", True)\n",
    "        .option(\"header\", True)\n",
    "        .option(\"sep\", ',')\n",
    "        .load('./base_statistics.csv')\n",
    ")\n",
    "loaded_df, loaded_df.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kxEu0kipR2jE",
    "mEuclFqwR5e0",
    "7SOyISvrSIun",
    "HCsO86XSW9kW",
    "lO38c_qmcYCT",
    "ybPKx2_0eJAG",
    "TtBLc8D6hJGb",
    "dfn4tqdooajR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
