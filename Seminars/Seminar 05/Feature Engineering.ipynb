{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 05: Feature Engineering`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Находнов Максим (nakhodnov17@gmail.com)`\n",
    "#### `Москва, 2023`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* Accumulator/Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:02.362387Z",
     "start_time": "2023-02-05T17:46:59.647723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/site-packages (from pyarrow) (1.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:13.832913Z",
     "start_time": "2023-02-05T17:47:04.417093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 20:34:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .setMaster('local[*]')\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Accumulator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = sc.accumulator(value=0)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, -4, 5])\n",
    "rdd.foreach(lambda x: acc.add(x))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_sum = sc.accumulator(0)\n",
    "\n",
    "def count(x):\n",
    "    global acc_sum\n",
    "    acc_sum += x\n",
    "    \n",
    "rdd.foreach(count)\n",
    "acc_sum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_cnt = sc.accumulator(0)\n",
    "rdd_02 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "rdd.foreach(lambda x: acc_cnt.add(1))\n",
    "rdd_02.foreach(lambda x: acc_cnt.add(1))\n",
    "acc_cnt.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return [0.0] * len(value)\n",
    "    \n",
    "    def addInPlace(self, value_left, value_right):\n",
    "        for idx in range(len(value_left)):\n",
    "             value_left[idx] += value_right[idx]\n",
    "        return value_left\n",
    "    \n",
    "vector_acc = sc.accumulator([1.0, 2.0, 3.0], VectorAccumulatorParam())\n",
    "vector_acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector_add(x):\n",
    "    global vector_acc\n",
    "    vector_acc += [x] * 3\n",
    "    \n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd.foreach(vector_add)\n",
    "vector_acc.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Broadcast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = {\"NY\": \"New York\", \"CA\": \"California\", \"FL\": \"Florida\"}\n",
    "broadcast_states = sc.broadcast(states)\n",
    "broadcast_states.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'USA', 'California'),\n",
       " ('Michael', 'Rose', 'USA', 'New York'),\n",
       " ('Robert', 'Williams', 'USA', 'California'),\n",
       " ('Maria', 'Jones', 'USA', 'Florida')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "    (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "    (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "    (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
    "]\n",
    "\n",
    "rdd_03 = sc.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcast_states.value[code]\n",
    "\n",
    "result = rdd_03.map(\n",
    "    lambda x: (x[0], x[1], x[2], state_convert(x[3]))\n",
    ").collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'USA', 'California'),\n",
       " ('Michael', 'Rose', 'USA', 'New York'),\n",
       " ('Robert', 'Williams', 'USA', 'California'),\n",
       " ('Maria', 'Jones', 'USA', 'Florida')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "    (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "    (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "    (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
    "]\n",
    "\n",
    "rdd_03 = sc.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return states[code]\n",
    "\n",
    "result = rdd_03.map(\n",
    "    lambda x: (x[0], x[1], x[2], state_convert(x[3]))\n",
    ").collect()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Broadcast JOIN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable broadcast Join and \n",
    "# Set Threshold limit of size in bytes of a DataFrame to broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "\n",
    "# Disable broadcast Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./m5-forecasting-accuracy\"\n",
    "# ! kaggle competitions download -c m5-forecasting-accuracy\n",
    "# ! unzip m5-forecasting-accuracy.zip -d $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Зададим пути к файлам из датасета\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "file_validation = f\"{path}/sales_train_validation.csv\"\n",
    "file_evaluation = f\"{path}/sales_train_evaluation.csv\"\n",
    "file_prices = f\"{path}/sell_prices.csv\"\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df_validation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_validation)\n",
    ")\n",
    "df_evaluation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_evaluation)\n",
    ")\n",
    "df_prices = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_prices)\n",
    ")\n",
    "df_calendar = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_calendar)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 20:34:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====>                                                   (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   cat_id|\n",
      "+---------+\n",
      "|    FOODS|\n",
      "|HOUSEHOLD|\n",
      "|  HOBBIES|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_evaluation.select(\n",
    "    df_evaluation.cat_id\n",
    ").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_hex =[\n",
    "    ('FOODS', '0x001'),\n",
    "    ('HOUSEHOLD', '0x002'),\n",
    "    ('HOBBIES', '0x003')\n",
    "]\n",
    "small_df = spark.createDataFrame(data=cat_id_hex, schema=['cat_id', 'hex_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|   cat_id|hex_code|\n",
      "+---------+--------+\n",
      "|    FOODS|   0x001|\n",
      "|HOUSEHOLD|   0x002|\n",
      "|  HOBBIES|   0x003|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>hex_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>0x003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1934  d_1935  d_1936  d_1937  d_1938  \\\n",
       "0       CA    0    0    0    0  ...       0       0       0       0       3   \n",
       "\n",
       "   d_1939  d_1940  d_1941   cat_id  hex_code  \n",
       "0       3       0       1  HOBBIES     0x003  \n",
       "\n",
       "[1 rows x 1949 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_join_df = df_evaluation.join(\n",
    "  F.broadcast(small_df), small_df.cat_id == df_evaluation.cat_id\n",
    ")\n",
    "broadcast_join_df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [cat_id#3875], [cat_id#9791], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(cat_id#3875)\n",
      "   :  +- FileScan csv [id#3872,item_id#3873,dept_id#3874,cat_id#3875,store_id#3876,state_id#3877,d_1#3878,d_2#3879,d_3#3880,d_4#3881,d_5#3882,d_6#3883,d_7#3884,d_8#3885,d_9#3886,d_10#3887,d_11#3888,d_12#3889,d_13#3890,d_14#3891,d_15#3892,d_16#3893,d_17#3894,d_18#3895,... 1923 more fields] Batched: false, DataFilters: [isnotnull(cat_id#3875)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/nakhodnov/HSE_DPO_Spark_2022/Seminars/Seminar 05/m5-foreca..., PartitionFilters: [], PushedFilters: [IsNotNull(cat_id)], ReadSchema: struct<id:string,item_id:string,dept_id:string,cat_id:string,store_id:string,state_id:string,d_1:...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=208]\n",
      "      +- Filter isnotnull(cat_id#9791)\n",
      "         +- Scan ExistingRDD[cat_id#9791,hex_code#9792]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast_join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [cat_id#3875], [cat_id#9791], Inner\n",
      "   :- Sort [cat_id#3875 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cat_id#3875, 200), ENSURE_REQUIREMENTS, [plan_id=232]\n",
      "   :     +- Filter isnotnull(cat_id#3875)\n",
      "   :        +- FileScan csv [id#3872,item_id#3873,dept_id#3874,cat_id#3875,store_id#3876,state_id#3877,d_1#3878,d_2#3879,d_3#3880,d_4#3881,d_5#3882,d_6#3883,d_7#3884,d_8#3885,d_9#3886,d_10#3887,d_11#3888,d_12#3889,d_13#3890,d_14#3891,d_15#3892,d_16#3893,d_17#3894,d_18#3895,... 1923 more fields] Batched: false, DataFilters: [isnotnull(cat_id#3875)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/nakhodnov/HSE_DPO_Spark_2022/Seminars/Seminar 05/m5-foreca..., PartitionFilters: [], PushedFilters: [IsNotNull(cat_id)], ReadSchema: struct<id:string,item_id:string,dept_id:string,cat_id:string,store_id:string,state_id:string,d_1:...\n",
      "   +- Sort [cat_id#9791 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(cat_id#9791, 200), ENSURE_REQUIREMENTS, [plan_id=233]\n",
      "         +- Filter isnotnull(cat_id#9791)\n",
      "            +- Scan ExistingRDD[cat_id#9791,hex_code#9792]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df_evaluation.join(\n",
    "  small_df, small_df.cat_id == df_evaluation.cat_id\n",
    ")\n",
    "join_df.explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark RDD Gradient Descent`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D = \\{(x_{i}, y_{i}) | x_{i} \\in \\mathbb{R}^{d}, y \\in \\mathbb{R}\\}_{1}^{n}\n",
    "$$\n",
    "$$\n",
    "\\hat{y}_{i} = \\langle x, w \\rangle + b\n",
    "$$\n",
    "$$\n",
    "L_{i} = \\frac{1}{2} (\\hat{y}_{i} - y_{i})^{2}\n",
    "$$\n",
    "$$\n",
    "\\mathfrak{L} = \\frac{1}{n}\\sum\\limits_{i=1}^{n} L_{i}\n",
    "$$\n",
    "\n",
    "Необходимо найти оптимальные $w \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из вариантов решения задачи: Градиентный Спуск (GD):\n",
    "\n",
    "$$\n",
    "w^{i+1} = w^{i} - \\alpha \\nabla_{w}\\mathfrak{L}\n",
    "$$\n",
    "$$\n",
    "b^{i+1} = b^{i} - \\alpha \\nabla_{b}\\mathfrak{L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(1000, 10)\n",
    "w_star = np.random.randn(X.shape[1])\n",
    "y = X.dot(w_star) + 0.001 * np.random.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.1, epochs=1):\n",
    "    # YOUR CODE HERE:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss 0.006\n",
      "Epoch: 3, Loss 0.003\n",
      "Epoch: 6, Loss 0.002\n",
      "Epoch: 9, Loss 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.94800571,  0.41386065,  0.66360389,  0.12221738,  1.15732219,\n",
       "        0.87192051, -0.23238663, -0.36209349, -0.97724333, -0.39244455])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X, y, alpha=0.1, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark Winsorizing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorizing(\n",
    "    df: pyspark.sql.dataframe.DataFrame, \n",
    "    lower_percentile: float = 0.1,\n",
    "    higher_percentile: float = 0.9\n",
    ") -> pyspark.sql.dataframe.DataFrame:\n",
    "    # YOUR CODE HERE:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      sales|\n",
      "+-----------+\n",
      "| 0.77347701|\n",
      "| 0.77617723|\n",
      "|-0.26191574|\n",
      "| 0.06015559|\n",
      "|-0.18058041|\n",
      "| 1.15605904|\n",
      "|-0.54163328|\n",
      "| 0.83280377|\n",
      "|-0.69920523|\n",
      "|-0.33986035|\n",
      "|-0.94114708|\n",
      "|-0.88438698|\n",
      "| 1.18682329|\n",
      "| 1.21287342|\n",
      "|-0.82575258|\n",
      "|  0.5895868|\n",
      "|  -1.646899|\n",
      "| -1.5341987|\n",
      "|-0.94135006|\n",
      "|  0.5699716|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (0.77347701,  ),\n",
    "    (0.77617723, ),\n",
    "    (-0.26191574,  ),\n",
    "    (0.06015559, ),\n",
    "    (-0.18058041,),\n",
    "    (1.15605904, ),\n",
    "    (-0.54163328,  ),\n",
    "    (0.83280377,),\n",
    "    (-0.69920523, ),\n",
    "    (-0.33986035,),\n",
    "    (-0.94114708, ),\n",
    "    (-0.88438698,  ),\n",
    "    (1.18682329,  ),\n",
    "    (1.21287342, ),\n",
    "    (-0.82575258,),\n",
    "    (0.5895868, ),\n",
    "    (-1.646899, ),\n",
    "    (-1.5341987, ),\n",
    "    (-0.94135006,  ),\n",
    "    (0.5699716,)\n",
    "]\n",
    "df = spark.createDataFrame(data, ['sales'])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 20:35:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/09 20:35:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/09 20:35:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/09 20:35:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/09 20:35:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-----------+-------------+-------------+--------------+\n",
      "|      sales|10_percentile|90_percentile|sales_winzored|\n",
      "+-----------+-------------+-------------+--------------+\n",
      "| 0.77347701|   -1.5341987|   1.15605904|    0.77347701|\n",
      "| 0.77617723|   -1.5341987|   1.15605904|    0.77617723|\n",
      "|-0.26191574|   -1.5341987|   1.15605904|   -0.26191574|\n",
      "| 0.06015559|   -1.5341987|   1.15605904|    0.06015559|\n",
      "|-0.18058041|   -1.5341987|   1.15605904|   -0.18058041|\n",
      "| 1.15605904|   -1.5341987|   1.15605904|    1.15605904|\n",
      "|-0.54163328|   -1.5341987|   1.15605904|   -0.54163328|\n",
      "| 0.83280377|   -1.5341987|   1.15605904|    0.83280377|\n",
      "|-0.69920523|   -1.5341987|   1.15605904|   -0.69920523|\n",
      "|-0.33986035|   -1.5341987|   1.15605904|   -0.33986035|\n",
      "|-0.94114708|   -1.5341987|   1.15605904|   -0.94114708|\n",
      "|-0.88438698|   -1.5341987|   1.15605904|   -0.88438698|\n",
      "| 1.18682329|   -1.5341987|   1.15605904|    1.15605904|\n",
      "| 1.21287342|   -1.5341987|   1.15605904|    1.15605904|\n",
      "|-0.82575258|   -1.5341987|   1.15605904|   -0.82575258|\n",
      "|  0.5895868|   -1.5341987|   1.15605904|     0.5895868|\n",
      "|  -1.646899|   -1.5341987|   1.15605904|    -1.5341987|\n",
      "| -1.5341987|   -1.5341987|   1.15605904|    -1.5341987|\n",
      "|-0.94135006|   -1.5341987|   1.15605904|   -0.94135006|\n",
      "|  0.5699716|   -1.5341987|   1.15605904|     0.5699716|\n",
      "+-----------+-------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "winsorizing(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
