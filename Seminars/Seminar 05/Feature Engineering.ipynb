{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 05: Feature Engineering`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Находнов Максим (nakhodnov17@gmail.com)`\n",
    "#### `Москва, 2023`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* Accumulator/Broadcast\n",
    "* Градиентный спуск\n",
    "* Винзоризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:02.362387Z",
     "start_time": "2023-02-05T17:46:59.647723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/site-packages (from pyarrow) (1.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:13.832913Z",
     "start_time": "2023-02-05T17:47:04.417093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/21 11:32:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .setMaster('local[*]')\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Общие данные в Spark`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе в распределённых окружениях нужно учитывать, что разные воркеры имеют прямой доступ только к своим локальным данным. Как следствие, алгоритмы в таких системах должны учитывать отсутствие общей памяти в целом и проблемы синхронизации между отдельными процессами в частности.\n",
    "\n",
    "Для решения этой проблемы в Spark предложены два средства: Accumulator и Broadcast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Accumulator`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аккумуляторы в Spark представляют из себя общую переменную, которые отдельные воркеры могут обновлять, но не могут считывать (так как значение этой переменной однозначно не определено по причине необходимости дорогостоящей синхронизации между отдельными процессами). \n",
    "\n",
    "Такие переменные поддерживают единственную операцию: `+=` (inplace add, `.__iadd__`) — коммутативную, ассоциативную операцию сложения. После того, как воркеры перестанут изменять переменную её значение можно получить на драйвере через атрибут `.value`.\n",
    "\n",
    "Пример использования аккумуляторов — подсчёт общих статистик в процессе вычислений, например, суммарное значение функции потерь (см. пример с GD ниже), общее число слов в датасете и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = sc.accumulator(value=0)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, -4, 5])\n",
    "rdd.foreach(lambda x: acc.add(x))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_sum = sc.accumulator(0)\n",
    "\n",
    "def count(x):\n",
    "    global acc_sum\n",
    "    acc_sum += x\n",
    "    \n",
    "rdd.foreach(count)\n",
    "acc_sum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_cnt = sc.accumulator(0)\n",
    "rdd_02 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "rdd.foreach(lambda x: acc_cnt.add(1))\n",
    "rdd_02.foreach(lambda x: acc_cnt.add(1))\n",
    "acc_cnt.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно определять аккумуляторы собственного типа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return [0.0] * len(value)\n",
    "    \n",
    "    def addInPlace(self, value_left, value_right):\n",
    "        for idx in range(len(value_left)):\n",
    "             value_left[idx] += value_right[idx]\n",
    "        return value_left\n",
    "    \n",
    "vector_acc = sc.accumulator([1.0, 2.0, 3.0], VectorAccumulatorParam())\n",
    "vector_acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector_add(x):\n",
    "    global vector_acc\n",
    "    vector_acc += [x] * 3\n",
    "    \n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd.foreach(vector_add)\n",
    "vector_acc.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Broadcast`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнением к WO общей памяти (аккумуляторам) являются RO переменные. Broadcast позволяет отправить на каждый воркер копию данных, которые затем можно использовать локально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = {\"NY\": \"New York\", \"CA\": \"California\", \"FL\": \"Florida\"}\n",
    "broadcast_states = sc.broadcast(states)\n",
    "broadcast_states.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'USA', 'California'),\n",
       " ('Michael', 'Rose', 'USA', 'New York'),\n",
       " ('Robert', 'Williams', 'USA', 'California'),\n",
       " ('Maria', 'Jones', 'USA', 'Florida')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "    (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "    (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "    (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
    "]\n",
    "\n",
    "rdd_03 = sc.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcast_states.value[code]\n",
    "\n",
    "result = rdd_03.map(\n",
    "    lambda x: (x[0], x[1], x[2], state_convert(x[3]))\n",
    ").collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'USA', 'California'),\n",
       " ('Michael', 'Rose', 'USA', 'New York'),\n",
       " ('Robert', 'Williams', 'USA', 'California'),\n",
       " ('Maria', 'Jones', 'USA', 'Florida')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "    (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "    (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "    (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
    "]\n",
    "\n",
    "rdd_03 = sc.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return states[code]\n",
    "\n",
    "result = rdd_03.map(\n",
    "    lambda x: (x[0], x[1], x[2], state_convert(x[3]))\n",
    ").collect()\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Broadcast JOIN`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Популярной вариантом использования Broadcast является объединение таблиц, одна из которых \"маленького\" размера. В таком случае может оказаться выгоднее отправить копию меньшей таблицы на каждый воркер и выполнить Join локально, нежели чем выполнять распределённое объединение таблиц. \n",
    "\n",
    "Нужно учитывать, что пересылка больших таблиц по сети может оказаться дорогостоящей, поэтому выбор между Broadcast Join и \"обычным\" Join зависит от конкретной конфигурации кластера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно задать размер DataFrame, при котором join будет автоматически происходить через broadcast этой таблицы\n",
    "# Размер задаётся в байтах. В данном случае — 100Мб.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "\n",
    "# Значение -1 отключает Broadcast Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./m5-forecasting-accuracy\"\n",
    "! kaggle competitions download -c m5-forecasting-accuracy\n",
    "! unzip m5-forecasting-accuracy.zip -d $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Зададим пути к файлам из датасета\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "file_validation = f\"{path}/sales_train_validation.csv\"\n",
    "file_evaluation = f\"{path}/sales_train_evaluation.csv\"\n",
    "file_prices = f\"{path}/sell_prices.csv\"\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df_validation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_validation)\n",
    ")\n",
    "df_evaluation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_evaluation)\n",
    ")\n",
    "df_prices = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_prices)\n",
    ")\n",
    "df_calendar = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_calendar)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/21 11:33:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   cat_id|\n",
      "+---------+\n",
      "|    FOODS|\n",
      "|HOUSEHOLD|\n",
      "|  HOBBIES|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_evaluation.select(\n",
    "    df_evaluation.cat_id\n",
    ").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|   cat_id|hex_code|\n",
      "+---------+--------+\n",
      "|    FOODS|   0x001|\n",
      "|HOUSEHOLD|   0x002|\n",
      "|  HOBBIES|   0x003|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_id_hex =[\n",
    "    ('FOODS', '0x001'),\n",
    "    ('HOUSEHOLD', '0x002'),\n",
    "    ('HOBBIES', '0x003')\n",
    "]\n",
    "small_df = spark.createDataFrame(data=cat_id_hex, schema=['cat_id', 'hex_code'])\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>hex_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>0x003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1934  d_1935  d_1936  d_1937  d_1938  \\\n",
       "0       CA    0    0    0    0  ...       0       0       0       0       3   \n",
       "\n",
       "   d_1939  d_1940  d_1941   cat_id  hex_code  \n",
       "0       3       0       1  HOBBIES     0x003  \n",
       "\n",
       "[1 rows x 1949 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_join_df = df_evaluation.join(\n",
    "  F.broadcast(small_df), small_df.cat_id == df_evaluation.cat_id\n",
    ")\n",
    "broadcast_join_df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [cat_id#3875], [cat_id#9791], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(cat_id#3875)\n",
      "   :  +- FileScan csv [id#3872,item_id#3873,dept_id#3874,cat_id#3875,store_id#3876,state_id#3877,d_1#3878,d_2#3879,d_3#3880,d_4#3881,d_5#3882,d_6#3883,d_7#3884,d_8#3885,d_9#3886,d_10#3887,d_11#3888,d_12#3889,d_13#3890,d_14#3891,d_15#3892,d_16#3893,d_17#3894,d_18#3895,... 1923 more fields] Batched: false, DataFilters: [isnotnull(cat_id#3875)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/nakhodnov/HSE_DPO_Spark_2022/Seminars/Seminar 05/m5-foreca..., PartitionFilters: [], PushedFilters: [IsNotNull(cat_id)], ReadSchema: struct<id:string,item_id:string,dept_id:string,cat_id:string,store_id:string,state_id:string,d_1:...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=208]\n",
      "      +- Filter isnotnull(cat_id#9791)\n",
      "         +- Scan ExistingRDD[cat_id#9791,hex_code#9792]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast_join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [cat_id#3875], [cat_id#9791], Inner\n",
      "   :- Sort [cat_id#3875 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cat_id#3875, 200), ENSURE_REQUIREMENTS, [plan_id=232]\n",
      "   :     +- Filter isnotnull(cat_id#3875)\n",
      "   :        +- FileScan csv [id#3872,item_id#3873,dept_id#3874,cat_id#3875,store_id#3876,state_id#3877,d_1#3878,d_2#3879,d_3#3880,d_4#3881,d_5#3882,d_6#3883,d_7#3884,d_8#3885,d_9#3886,d_10#3887,d_11#3888,d_12#3889,d_13#3890,d_14#3891,d_15#3892,d_16#3893,d_17#3894,d_18#3895,... 1923 more fields] Batched: false, DataFilters: [isnotnull(cat_id#3875)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/nakhodnov/HSE_DPO_Spark_2022/Seminars/Seminar 05/m5-foreca..., PartitionFilters: [], PushedFilters: [IsNotNull(cat_id)], ReadSchema: struct<id:string,item_id:string,dept_id:string,cat_id:string,store_id:string,state_id:string,d_1:...\n",
      "   +- Sort [cat_id#9791 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(cat_id#9791, 200), ENSURE_REQUIREMENTS, [plan_id=233]\n",
      "         +- Filter isnotnull(cat_id#9791)\n",
      "            +- Scan ExistingRDD[cat_id#9791,hex_code#9792]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df_evaluation.join(\n",
    "  small_df, small_df.cat_id == df_evaluation.cat_id\n",
    ")\n",
    "join_df.explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark RDD Gradient Descent`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D = \\{(x_{i}, y_{i}) | x_{i} \\in \\mathbb{R}^{d}, y \\in \\mathbb{R}\\}_{1}^{n}\n",
    "$$\n",
    "$$\n",
    "\\hat{y}_{i} = \\langle x, w \\rangle + b\n",
    "$$\n",
    "$$\n",
    "L_{i} = \\frac{1}{2} (\\hat{y}_{i} - y_{i})^{2}\n",
    "$$\n",
    "$$\n",
    "\\mathfrak{L}(w, b) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} L_{i} \\longrightarrow \\min_{w, b}\n",
    "$$\n",
    "\n",
    "Необходимо найти оптимальные $w \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из вариантов решения задачи: Градиентный Спуск (GD):\n",
    "\n",
    "$$\n",
    "w^{i+1} = w^{i} - \\alpha \\nabla_{w}\\mathfrak{L}\n",
    "$$\n",
    "$$\n",
    "b^{i+1} = b^{i} - \\alpha \\nabla_{b}\\mathfrak{L}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель данных:\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}(\\langle x_{i}, w^{*} \\rangle, \\sigma^{2})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{w} L = (\\frac{\\partial L}{\\partial w_{1}}, ..., \\frac{\\partial L}{\\partial w_{d}})$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{w} \\mathfrak{L} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\nabla_{w}(\\hat{y}_{i} - y_{i})(\\hat{y}_{i} - y_{i}) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (\\hat{y}_{i} - y_{i}) \\nabla_{w}\\hat{y}_{i} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (\\hat{y}_{i} - y_{i}) x_{i} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (\\langle x_{i}, w \\rangle - y_{i}) x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(1000, 10)\n",
    "w_star = np.random.randn(X.shape[1])\n",
    "y = X.dot(w_star) + 0.001 * np.random.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.1, epochs=1):\n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Init w\n",
    "    # 2. In loop repeat:\n",
    "    #     a. Calculate error, gradient\n",
    "    #     b. Update w\n",
    "    # 3. Return w\n",
    "    \n",
    "    X_rdd = sc.parallelize(X).cache()\n",
    "    y_rdd = sc.parallelize(y).cache()\n",
    "    n = X_rdd.count()\n",
    "    d = X_rdd.take(1)[0].shape[0]\n",
    "    \n",
    "    # Кэшируем результат вычислений, чтобы не перевычислять его на каждой итерации\n",
    "    X_y_rdd = X_rdd.zip(y_rdd).cache()\n",
    "    w = np.zeros(d)\n",
    "    for epoch in range(epochs):\n",
    "        w_br = sc.broadcast(w)\n",
    "        n_br = sc.broadcast(n)\n",
    "        \n",
    "        total_error = sc.accumulator(0.0)\n",
    "        def grad_mapper(x_y, total_error):\n",
    "            delta = (np.sum(x_y[0] * w_br.value) - x_y[1])\n",
    "            error = (delta ** 2.0) / 2.0\n",
    "            total_error.add(error / n_br.value)\n",
    "            return x_y[0] * delta\n",
    "        \n",
    "        grad = X_y_rdd.map(partial(grad_mapper, total_error=total_error)).sum() / n\n",
    "        w = w - alpha * grad\n",
    "        \n",
    "        if epoch % 3 == 0:\n",
    "            print('Epoch: {0:d}, Loss {1:.3f}'.format(epoch, total_error.value / n))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss 0.006\n",
      "Epoch: 3, Loss 0.003\n",
      "Epoch: 6, Loss 0.002\n",
      "Epoch: 9, Loss 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.42157966, -0.11659553, -0.68411146, -1.03660564, -0.80783741,\n",
       "       -0.68547898,  1.01404414,  0.19389612, -0.18162573,  1.01462433])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X, y, alpha=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56935807, -0.12347339, -0.99334668, -1.63083152, -1.24828304,\n",
       "       -1.01660745,  1.57347421,  0.19636134, -0.25449749,  1.57294227])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_star"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark Winsorizing`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Винзоризация — это метод предобработки численных данных, при котором значения за пределами заданных квантилей заменяются на значения этих квантилей. \n",
    "\n",
    "Например, $X = \\{100, 6, 52, 26, 8, 81, 52, 15, 2, 74, 93, 82, 36, 22, 74, 90, 97, 50, 4, 40, 1\\}$.\n",
    "\n",
    "Винзоризация для $0.1$ и $0.9$ квантилей выполняется следующим образом:\n",
    "1. Определяем квантили: $q_{0.1} = 3, q_{0.9} = 93$\n",
    "2. Заменяем все значения меньше $3$ на $3$ и больше $93$ на $93$: \n",
    "\n",
    "$$\\hat{X} = \\{93, 6, 52, 26, 8, 81, 52, 15, 3, 74, 93, 82, 36, 22, 74, 90, 97, 50, 4, 40, 3\\}$$\n",
    "\n",
    "Такая предобработка убирает экстремальные значения и выбросы, что приводит к более надёжному посчёту статистик по выборке (матожидание, дисперисия и так далее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorizing(\n",
    "    df: pyspark.sql.dataframe.DataFrame, \n",
    "    column: str = 'sales',\n",
    "    lower_percentile: float = 0.1,\n",
    "    higher_percentile: float = 0.9\n",
    ") -> pyspark.sql.dataframe.DataFrame:\n",
    "    # YOUR CODE HERE:\n",
    "    \n",
    "    wspec = Window().partitionBy()\n",
    "    \n",
    "    lp_column = '_'.join([column, 'lower_percentile'])\n",
    "    hp_column = '_'.join([column, 'higher_percentile'])\n",
    "    df = df.withColumns({\n",
    "        lp_column: F.percentile_approx(F.col(column), lower_percentile).over(wspec),\n",
    "        hp_column: F.percentile_approx(F.col(column), higher_percentile).over(wspec)\n",
    "    })\n",
    "    \n",
    "    df = (\n",
    "        df\n",
    "            .withColumn(\n",
    "                '_'.join([column, 'winsorized']),\n",
    "                F.when(\n",
    "                    F.col(column) < F.col(lp_column),\n",
    "                    F.col(lp_column)\n",
    "                ).otherwise(\n",
    "                    F.when(\n",
    "                        F.col(column) > F.col(hp_column),\n",
    "                        F.col(hp_column)\n",
    "                    ).otherwise(\n",
    "                        F.col(column)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      sales|\n",
      "+-----------+\n",
      "| 0.77347701|\n",
      "| 0.77617723|\n",
      "|-0.26191574|\n",
      "| 0.06015559|\n",
      "|-0.18058041|\n",
      "| 1.15605904|\n",
      "|-0.54163328|\n",
      "| 0.83280377|\n",
      "|-0.69920523|\n",
      "|-0.33986035|\n",
      "|-0.94114708|\n",
      "|-0.88438698|\n",
      "| 1.18682329|\n",
      "| 1.21287342|\n",
      "|-0.82575258|\n",
      "|  0.5895868|\n",
      "|  -1.646899|\n",
      "| -1.5341987|\n",
      "|-0.94135006|\n",
      "|  0.5699716|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (0.77347701,  ),\n",
    "    (0.77617723, ),\n",
    "    (-0.26191574,  ),\n",
    "    (0.06015559, ),\n",
    "    (-0.18058041,),\n",
    "    (1.15605904, ),\n",
    "    (-0.54163328,  ),\n",
    "    (0.83280377,),\n",
    "    (-0.69920523, ),\n",
    "    (-0.33986035,),\n",
    "    (-0.94114708, ),\n",
    "    (-0.88438698,  ),\n",
    "    (1.18682329,  ),\n",
    "    (1.21287342, ),\n",
    "    (-0.82575258,),\n",
    "    (0.5895868, ),\n",
    "    (-1.646899, ),\n",
    "    (-1.5341987, ),\n",
    "    (-0.94135006,  ),\n",
    "    (0.5699716,)\n",
    "]\n",
    "df = spark.createDataFrame(data, ['sales'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-----------+----------------------+-----------------------+----------------+\n",
      "|      sales|sales_lower_percentile|sales_higher_percentile|sales_winsorized|\n",
      "+-----------+----------------------+-----------------------+----------------+\n",
      "| 0.77347701|            -1.5341987|             1.15605904|      0.77347701|\n",
      "| 0.77617723|            -1.5341987|             1.15605904|      0.77617723|\n",
      "|-0.26191574|            -1.5341987|             1.15605904|     -0.26191574|\n",
      "| 0.06015559|            -1.5341987|             1.15605904|      0.06015559|\n",
      "|-0.18058041|            -1.5341987|             1.15605904|     -0.18058041|\n",
      "| 1.15605904|            -1.5341987|             1.15605904|      1.15605904|\n",
      "|-0.54163328|            -1.5341987|             1.15605904|     -0.54163328|\n",
      "| 0.83280377|            -1.5341987|             1.15605904|      0.83280377|\n",
      "|-0.69920523|            -1.5341987|             1.15605904|     -0.69920523|\n",
      "|-0.33986035|            -1.5341987|             1.15605904|     -0.33986035|\n",
      "|-0.94114708|            -1.5341987|             1.15605904|     -0.94114708|\n",
      "|-0.88438698|            -1.5341987|             1.15605904|     -0.88438698|\n",
      "| 1.18682329|            -1.5341987|             1.15605904|      1.15605904|\n",
      "| 1.21287342|            -1.5341987|             1.15605904|      1.15605904|\n",
      "|-0.82575258|            -1.5341987|             1.15605904|     -0.82575258|\n",
      "|  0.5895868|            -1.5341987|             1.15605904|       0.5895868|\n",
      "|  -1.646899|            -1.5341987|             1.15605904|      -1.5341987|\n",
      "| -1.5341987|            -1.5341987|             1.15605904|      -1.5341987|\n",
      "|-0.94135006|            -1.5341987|             1.15605904|     -0.94135006|\n",
      "|  0.5699716|            -1.5341987|             1.15605904|       0.5699716|\n",
      "+-----------+----------------------+-----------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "winsorizing(df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
