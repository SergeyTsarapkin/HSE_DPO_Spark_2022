{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 18:14:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .setMaster('local[*]')\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfn4tqdooajR"
   },
   "source": [
    "### Spark RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6XuSx2Eo16N"
   },
   "source": [
    "* [Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "* [Документация](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oFdQjWbfouNJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 10),\n",
    "    (2, 41),\n",
    "    (0, 12),\n",
    "    (2, 64),\n",
    "    (2, 22),\n",
    "    (1, 11),\n",
    "    (0, 94),\n",
    "]\n",
    "dist_data = sc.parallelize(data)\n",
    "dist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWDkHLUbpRrA",
    "outputId": "186df13f-6485-4e3b-8b6a-c53fd23673a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10), (2, 41), (0, 12), (2, 64), (2, 22), (1, 11), (0, 94)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8lXGP8NpTOM",
    "outputId": "f2e5c344-9986-4955-c5bf-0c64b3f0bc95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 43, 12, 66, 24, 12, 94]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.map(lambda x: x[0] + x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NivHzNFppYVs",
    "outputId": "c79c44b6-c649-436f-8931-9e7812eda325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[key: bigint, value: bigint],\n",
       " [Row(key=1, value=10),\n",
       "  Row(key=2, value=41),\n",
       "  Row(key=0, value=12),\n",
       "  Row(key=2, value=64),\n",
       "  Row(key=2, value=22),\n",
       "  Row(key=1, value=11),\n",
       "  Row(key=0, value=94)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df = dist_data.toDF(['key', 'value'])\n",
    "dist_df, dist_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LeiWR9VptZU",
    "outputId": "1e9f535f-7097-4cfd-8c51-fd445a849039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 106), (1, 21), (2, 127)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.groupByKey().mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oNixwVHXrkfy"
   },
   "outputs": [],
   "source": [
    "! echo \"Hello, sample RDD\" > text.txt\n",
    "! echo \"This RDD contains three lines\" >> text.txt\n",
    "! echo \"This is the last line\" >> text.txt\n",
    "! echo \"\" >> text.txt\n",
    "! echo \"Just kidding, it contains five lines\" >> text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXaOLQ7ksEnO",
    "outputId": "4f5d3461-2625-406a-e550-42198a9c029f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(text.txt MapPartitionsRDD[16] at textFile at DirectMethodHandleAccessor.java:104,\n",
       " ['Hello, sample RDD',\n",
       "  'This RDD contains three lines',\n",
       "  'This is the last line',\n",
       "  '',\n",
       "  'Just kidding, it contains five lines'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = sc.textFile('text.txt')\n",
    "text_data, text_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PoNCwEfFsHvF"
   },
   "outputs": [],
   "source": [
    "distinct_words = text_data.filter(lambda x: len(x)).flatMap(lambda x: x.split(' ')).distinct()\n",
    "distinct_words.saveAsTextFile('words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[20] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[19] at partitionBy at DirectMethodHandleAccessor.java:104 []\n",
      " +-(2) PairwiseRDD[18] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/4217843450.py:1 []\n",
      "    |  PythonRDD[17] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/4217843450.py:1 []\n",
      "    |  text.txt MapPartitionsRDD[16] at textFile at DirectMethodHandleAccessor.java:104 []\n",
      "    |  text.txt HadoopRDD[15] at textFile at DirectMethodHandleAccessor.java:104 []\n"
     ]
    }
   ],
   "source": [
    "print(distinct_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[24] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |  MapPartitionsRDD[20] at mapPartitions at PythonRDD.scala:145 [Memory Serialized 1x Replicated]\n",
      " |  ShuffledRDD[19] at partitionBy at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      " +-(2) PairwiseRDD[18] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/4217843450.py:1 [Memory Serialized 1x Replicated]\n",
      "    |  PythonRDD[17] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/4217843450.py:1 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt MapPartitionsRDD[16] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt HadoopRDD[15] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "distinct_words_cached = distinct_words.cache()\n",
    "print(distinct_words_cached.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[24] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |       CachedPartitions: 2; MemorySize: 298.0 B; DiskSize: 0.0 B\n",
      " |  MapPartitionsRDD[20] at mapPartitions at PythonRDD.scala:145 [Memory Serialized 1x Replicated]\n",
      " |  ShuffledRDD[19] at partitionBy at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      " +-(2) PairwiseRDD[18] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/4217843450.py:1 [Memory Serialized 1x Replicated]\n",
      "    |  PythonRDD[17] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/4217843450.py:1 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt MapPartitionsRDD[16] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt HadoopRDD[15] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "distinct_words_cached.collect()\n",
    "print(distinct_words_cached.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[29] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[28] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[27] at partitionBy at DirectMethodHandleAccessor.java:104 []\n",
      " +-(2) PairwiseRDD[26] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/1948573496.py:1 []\n",
      "    |  PythonRDD[25] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/1948573496.py:1 []\n",
      "    |  text.txt MapPartitionsRDD[16] at textFile at DirectMethodHandleAccessor.java:104 []\n",
      "    |  text.txt HadoopRDD[15] at textFile at DirectMethodHandleAccessor.java:104 []\n"
     ]
    }
   ],
   "source": [
    "distinct_first_words = text_data.filter(lambda x: len(x)).flatMap(lambda x: x.split(' ')[0]).distinct()\n",
    "\n",
    "sc.setCheckpointDir('./checkpoints')\n",
    "\n",
    "distinct_first_words.checkpoint()\n",
    "print(distinct_first_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[29] at RDD at PythonRDD.scala:53 []\n",
      " |  ReliableCheckpointRDD[30] at collect at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_56393/656455338.py:1 []\n"
     ]
    }
   ],
   "source": [
    "distinct_first_words.collect()\n",
    "print(distinct_first_words.toDebugString().decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot/Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-02 16:33:39--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Распознаётся files.grouplens.org (files.grouplens.org)… 128.101.65.152\n",
      "Подключение к files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 5917549 (5,6M) [application/zip]\n",
      "Сохранение в: «ml-1m.zip»\n",
      "\n",
      "ml-1m.zip           100%[===================>]   5,64M  2,44MB/s    за 2,3s    \n",
      "\n",
      "2023-02-02 16:33:42 (2,44 MB/s) - «ml-1m.zip» сохранён [5917549/5917549]\n",
      "\n",
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat        \n",
      "  inflating: ml-1m/ratings.dat       \n",
      "  inflating: ml-1m/README            \n",
      "  inflating: ml-1m/users.dat         \n"
     ]
    }
   ],
   "source": [
    "! wget https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "! unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('movie_id', T.IntegerType())\n",
    "        .add('movie', T.StringType())\n",
    "        .add('categories', T.StringType())\n",
    ")\n",
    "movies_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/movies.dat')\n",
    ")\n",
    "\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('user_id', T.IntegerType())\n",
    "        .add('movie_id', T.IntegerType())\n",
    "        .add('rating', T.FloatType())\n",
    "        .add('timestamp', T.StringType())\n",
    ")\n",
    "ratings_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/ratings.dat')\n",
    ")\n",
    "\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('user_id', T.IntegerType())\n",
    "        .add('gender', T.StringType())\n",
    "        .add('age', T.IntegerType())\n",
    "        .add('occupation', T.IntegerType())\n",
    "        .add('zip-code', T.StringType())\n",
    ")\n",
    "users_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/users.dat')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|      1|    1193|   5.0|978300760|\n",
      "|      1|     661|   3.0|978302109|\n",
      "|      1|     914|   3.0|978301968|\n",
      "|      1|    3408|   4.0|978300275|\n",
      "|      1|    2355|   5.0|978824291|\n",
      "|      1|    1197|   3.0|978302268|\n",
      "|      1|    1287|   5.0|978302039|\n",
      "|      1|    2804|   5.0|978300719|\n",
      "|      1|     594|   4.0|978302268|\n",
      "|      1|     919|   4.0|978301368|\n",
      "|      1|     595|   5.0|978824268|\n",
      "|      1|     938|   4.0|978301752|\n",
      "|      1|    2398|   4.0|978302281|\n",
      "|      1|    2918|   4.0|978302124|\n",
      "|      1|    1035|   5.0|978301753|\n",
      "|      1|    2791|   4.0|978302188|\n",
      "|      1|    2687|   3.0|978824268|\n",
      "|      1|    2018|   4.0|978301777|\n",
      "|      1|    3105|   5.0|978301713|\n",
      "|      1|    2797|   4.0|978302039|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 18:14:50 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 18:14:53 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3943</th>\n",
       "      <th>3944</th>\n",
       "      <th>3945</th>\n",
       "      <th>3946</th>\n",
       "      <th>3947</th>\n",
       "      <th>3948</th>\n",
       "      <th>3949</th>\n",
       "      <th>3950</th>\n",
       "      <th>3951</th>\n",
       "      <th>3952</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 3707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    1   2   3   4   5   6   7   8   9  ...  3943  3944  3945  3946  \\\n",
       "0        1  5.0 NaN NaN NaN NaN NaN NaN NaN NaN  ...   NaN   NaN   NaN   NaN   \n",
       "1        2  NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   3947  3948  3949  3950  3951  3952  \n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[2 rows x 3707 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df = ratings_df.groupBy(ratings_df.user_id).pivot('movie_id').agg(F.first(ratings_df.rating))\n",
    "pivot_df.where(pivot_df.user_id < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------------+\n",
      "|movie_id|rates|        avg_rating|\n",
      "+--------+-----+------------------+\n",
      "|    2858| 3428|4.3173862310385065|\n",
      "|     260| 2991| 4.453694416583082|\n",
      "|    1196| 2990| 4.292976588628763|\n",
      "|    1210| 2883| 4.022892819979188|\n",
      "|     480| 2672|3.7638473053892216|\n",
      "|    2028| 2653| 4.337353938937053|\n",
      "|     589| 2649| 4.058512646281616|\n",
      "|    2571| 2590| 4.315830115830116|\n",
      "|    1270| 2583|3.9903213317847466|\n",
      "|     593| 2578|4.3518231186966645|\n",
      "|    1580| 2538| 3.739952718676123|\n",
      "|    1198| 2514| 4.477724741447892|\n",
      "|     608| 2513| 4.254675686430561|\n",
      "|    2762| 2459| 4.406262708418057|\n",
      "|     110| 2443| 4.234957020057307|\n",
      "|    2396| 2369| 4.127479949345715|\n",
      "|    1197| 2318|4.3037100949094045|\n",
      "|     527| 2304| 4.510416666666667|\n",
      "|    1617| 2288| 4.219405594405594|\n",
      "|    1265| 2278| 3.953028972783143|\n",
      "+--------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_movies_df = ratings_df.groupBy(ratings_df.movie_id).agg(\n",
    "    F.count(ratings_df.rating).alias('rates'),\n",
    "    F.mean(ratings_df.rating).alias('avg_rating')\n",
    ").sort('rates', ascending=False).limit(100)\n",
    "\n",
    "top_movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = top_movies_df.rdd.map(lambda x: x.movie_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>2858</th>\n",
       "      <th>260</th>\n",
       "      <th>1196</th>\n",
       "      <th>1210</th>\n",
       "      <th>480</th>\n",
       "      <th>2028</th>\n",
       "      <th>589</th>\n",
       "      <th>2571</th>\n",
       "      <th>1270</th>\n",
       "      <th>...</th>\n",
       "      <th>2699</th>\n",
       "      <th>750</th>\n",
       "      <th>39</th>\n",
       "      <th>21</th>\n",
       "      <th>1393</th>\n",
       "      <th>2804</th>\n",
       "      <th>588</th>\n",
       "      <th>2406</th>\n",
       "      <th>1220</th>\n",
       "      <th>733</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  2858  260  1196  1210  480  2028  589  2571  1270  ...  2699  750  \\\n",
       "0        1   NaN  4.0   NaN   NaN  NaN   5.0  NaN   NaN   5.0  ...   NaN  NaN   \n",
       "1        2   4.0  NaN   5.0   4.0  5.0   4.0  4.0   4.0   NaN  ...   NaN  NaN   \n",
       "\n",
       "   39   21  1393  2804  588  2406  1220  733  \n",
       "0 NaN  NaN   NaN   5.0  4.0   NaN   NaN  NaN  \n",
       "1 NaN  1.0   NaN   NaN  NaN   NaN   NaN  NaN  \n",
       "\n",
       "[2 rows x 101 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_top_df = ratings_df.groupBy(ratings_df.user_id).pivot('movie_id', top_movies).agg(F.first(ratings_df.rating))\n",
    "pivot_top_df.where(pivot_df.user_id < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>2858</th>\n",
       "      <th>260</th>\n",
       "      <th>1196</th>\n",
       "      <th>1210</th>\n",
       "      <th>480</th>\n",
       "      <th>2028</th>\n",
       "      <th>589</th>\n",
       "      <th>2571</th>\n",
       "      <th>1270</th>\n",
       "      <th>...</th>\n",
       "      <th>2699</th>\n",
       "      <th>750</th>\n",
       "      <th>39</th>\n",
       "      <th>21</th>\n",
       "      <th>1393</th>\n",
       "      <th>2804</th>\n",
       "      <th>588</th>\n",
       "      <th>2406</th>\n",
       "      <th>1220</th>\n",
       "      <th>733</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  2858  260  1196  1210  480  2028  589  2571  1270  ...  2699  750  \\\n",
       "0        1   3.0  4.0   3.0   3.0  3.0   5.0  3.0   3.0   5.0  ...   3.0  3.0   \n",
       "1        2   4.0  3.0   5.0   4.0  5.0   4.0  4.0   4.0   3.0  ...   3.0  3.0   \n",
       "\n",
       "    39   21  1393  2804  588  2406  1220  733  \n",
       "0  3.0  3.0   3.0   5.0  4.0   3.0   3.0  3.0  \n",
       "1  3.0  1.0   3.0   3.0  3.0   3.0   3.0  3.0  \n",
       "\n",
       "[2 rows x 101 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_top_df = ratings_df.groupBy(ratings_df.user_id).pivot('movie_id', top_movies).agg(\n",
    "    F.first(ratings_df.rating)\n",
    ").fillna(3.0)\n",
    "pivot_top_df.where(pivot_top_df.user_id < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "5  HOBBIES_1_006_CA_1_validation  HOBBIES_1_006  HOBBIES_1  HOBBIES     CA_1   \n",
       "6  HOBBIES_1_007_CA_1_validation  HOBBIES_1_007  HOBBIES_1  HOBBIES     CA_1   \n",
       "7  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "8  HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "9  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "5       CA    0    0    0    0  ...       0       1       0       1       0   \n",
       "6       CA    0    0    0    0  ...       0       0       0       1       0   \n",
       "7       CA   12   15    0    0  ...       0       0       1      37       3   \n",
       "8       CA    2    0    7    3  ...       0       0       1       1       6   \n",
       "9       CA    0    0    1    0  ...       1       0       0       0       0   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "5       0       0       2       0       0  \n",
       "6       1       0       0       1       1  \n",
       "7       4       6       3       2       1  \n",
       "8       0       0       0       0       0  \n",
       "9       0       0       2       0       2  \n",
       "\n",
       "[10 rows x 1919 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./m5-forecasting-accuracy\"\n",
    "\n",
    "df_validation = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"inferSchema\", True)\n",
    "      .option(\"header\", True)\n",
    "      .option(\"sep\", ',')\n",
    "      .load(f\"{path}/sales_train_validation.csv\")\n",
    ")\n",
    "df_validation.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---+-----+\n",
      "|id                           |d  |sales|\n",
      "+-----------------------------+---+-----+\n",
      "|HOBBIES_1_001_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_001_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_002_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_002_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_003_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_003_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_004_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_004_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_005_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_005_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_006_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_006_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_007_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_007_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_008_CA_1_validation|d_1|12   |\n",
      "|HOBBIES_1_008_CA_1_validation|d_2|15   |\n",
      "|HOBBIES_1_009_CA_1_validation|d_1|2    |\n",
      "|HOBBIES_1_009_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_010_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_010_CA_1_validation|d_2|0    |\n",
      "+-----------------------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_expression = \"stack(2, 'd_1', d_1, 'd_2', d_2) as (d, sales)\"\n",
    "unpivot_df = (\n",
    "    df_validation\n",
    "        .select('id', F.expr(unpivot_expression))\n",
    ")\n",
    "\n",
    "unpivot_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---+---+\n",
      "|id                             |d_1|d_2|\n",
      "+-------------------------------+---+---+\n",
      "|FOODS_2_387_CA_1_validation    |0  |0  |\n",
      "|HOBBIES_1_258_CA_1_validation  |0  |0  |\n",
      "|FOODS_2_322_CA_1_validation    |0  |1  |\n",
      "|FOODS_3_352_CA_1_validation    |0  |0  |\n",
      "|FOODS_1_011_CA_1_validation    |2  |1  |\n",
      "|HOBBIES_1_273_CA_1_validation  |1  |0  |\n",
      "|HOBBIES_1_163_CA_1_validation  |0  |0  |\n",
      "|FOODS_2_011_CA_1_validation    |1  |1  |\n",
      "|FOODS_1_101_CA_2_validation    |0  |0  |\n",
      "|HOBBIES_2_044_CA_2_validation  |0  |0  |\n",
      "|FOODS_3_808_CA_1_validation    |22 |18 |\n",
      "|HOUSEHOLD_1_179_CA_2_validation|9  |5  |\n",
      "|HOBBIES_1_236_CA_2_validation  |0  |0  |\n",
      "|HOBBIES_1_299_CA_2_validation  |0  |0  |\n",
      "|FOODS_1_206_CA_2_validation    |3  |0  |\n",
      "|FOODS_3_529_CA_1_validation    |0  |0  |\n",
      "|FOODS_3_644_CA_1_validation    |3  |1  |\n",
      "|FOODS_3_693_CA_1_validation    |0  |0  |\n",
      "|FOODS_1_054_CA_2_validation    |4  |6  |\n",
      "|HOUSEHOLD_1_157_CA_2_validation|0  |0  |\n",
      "+-------------------------------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_df.groupBy(unpivot_df.id).pivot('d').agg(F.sum(unpivot_df.sales)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---+---+\n",
      "|id                         |d_1|d_2|\n",
      "+---------------------------+---+---+\n",
      "|FOODS_3_808_CA_1_validation|22 |18 |\n",
      "+---------------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_validation.where(df_validation.id == 'FOODS_3_808_CA_1_validation').select('id', 'd_1', 'd_2').show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------+---------+------+\n",
      "|emp_id |dept_id  |salary|\n",
      "+-------+---------+------+\n",
      "|James  |Sales    |3000  |\n",
      "|Michael|Sales    |4600  |\n",
      "|Robert |Sales    |4100  |\n",
      "|Maria  |Finance  |3000  |\n",
      "|James  |Sales    |3000  |\n",
      "|Scott  |Finance  |3300  |\n",
      "|Jen    |Finance  |3900  |\n",
      "|Jeff   |Marketing|3000  |\n",
      "|Kumar  |Marketing|2000  |\n",
      "|Saif   |Sales    |4100  |\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Michael', 'Sales', 4600),\n",
    "    ('Robert', 'Sales', 4100),\n",
    "    ('Maria', 'Finance', 3000),\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Scott', 'Finance', 3300),\n",
    "    ('Jen', 'Finance', 3900), \n",
    "    ('Jeff', 'Marketing', 3000),\n",
    "    ('Kumar', 'Marketing', 2000),\n",
    "    ('Saif', 'Sales', 4100)\n",
    "]\n",
    " \n",
    "columns= [\"emp_id\", \"dept_id\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema(), df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+----------+\n",
      "| emp_id|  dept_id|salary|       salaries_list|avg_salary|\n",
      "+-------+---------+------+--------------------+----------+\n",
      "|  Maria|  Finance|  3000|  [3000, 3300, 3900]|    3400.0|\n",
      "|  Scott|  Finance|  3300|  [3000, 3300, 3900]|    3400.0|\n",
      "|    Jen|  Finance|  3900|  [3000, 3300, 3900]|    3400.0|\n",
      "|   Jeff|Marketing|  3000|        [3000, 2000]|    2500.0|\n",
      "|  Kumar|Marketing|  2000|        [3000, 2000]|    2500.0|\n",
      "|  James|    Sales|  3000|[3000, 4600, 4100...|    3760.0|\n",
      "|Michael|    Sales|  4600|[3000, 4600, 4100...|    3760.0|\n",
      "| Robert|    Sales|  4100|[3000, 4600, 4100...|    3760.0|\n",
      "|  James|    Sales|  3000|[3000, 4600, 4100...|    3760.0|\n",
      "|   Saif|    Sales|  4100|[3000, 4600, 4100...|    3760.0|\n",
      "+-------+---------+------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id')\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+----------+----------+\n",
      "| emp_id|  dept_id|salary|       salaries_list|row_number|avg_salary|\n",
      "+-------+---------+------+--------------------+----------+----------+\n",
      "|  Maria|  Finance|  3000|              [3000]|         1|    3000.0|\n",
      "|  Scott|  Finance|  3300|        [3000, 3300]|         2|    3150.0|\n",
      "|    Jen|  Finance|  3900|  [3000, 3300, 3900]|         3|    3400.0|\n",
      "|  Kumar|Marketing|  2000|              [2000]|         1|    2000.0|\n",
      "|   Jeff|Marketing|  3000|        [2000, 3000]|         2|    2500.0|\n",
      "|  James|    Sales|  3000|        [3000, 3000]|         1|    3000.0|\n",
      "|  James|    Sales|  3000|        [3000, 3000]|         2|    3000.0|\n",
      "| Robert|    Sales|  4100|[3000, 3000, 4100...|         3|    3550.0|\n",
      "|   Saif|    Sales|  4100|[3000, 3000, 4100...|         4|    3550.0|\n",
      "|Michael|    Sales|  4600|[3000, 3000, 4100...|         5|    3760.0|\n",
      "+-------+---------+------+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary')\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('row_number', F.row_number().over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+-----------------+\n",
      "| emp_id|  dept_id|salary|       salaries_list|       avg_salary|\n",
      "+-------+---------+------+--------------------+-----------------+\n",
      "|  Maria|  Finance|  3000|  [3000, 3300, 3900]|           3400.0|\n",
      "|  Scott|  Finance|  3300|        [3300, 3900]|           3600.0|\n",
      "|    Jen|  Finance|  3900|              [3900]|           3900.0|\n",
      "|  Kumar|Marketing|  2000|        [2000, 3000]|           2500.0|\n",
      "|   Jeff|Marketing|  3000|              [3000]|           3000.0|\n",
      "|  James|    Sales|  3000|[3000, 3000, 4100...|           3760.0|\n",
      "|  James|    Sales|  3000|[3000, 4100, 4100...|           3950.0|\n",
      "| Robert|    Sales|  4100|  [4100, 4100, 4600]|4266.666666666667|\n",
      "|   Saif|    Sales|  4100|        [4100, 4600]|           4350.0|\n",
      "|Michael|    Sales|  4600|              [4600]|           4600.0|\n",
      "+-------+---------+------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary').rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-------------+----------+\n",
      "| emp_id|  dept_id|salary|salaries_list|avg_salary|\n",
      "+-------+---------+------+-------------+----------+\n",
      "|  Maria|  Finance|  3000| [3000, 3300]|    3150.0|\n",
      "|  Scott|  Finance|  3300| [3000, 3300]|    3150.0|\n",
      "|    Jen|  Finance|  3900|       [3900]|    3900.0|\n",
      "|  Kumar|Marketing|  2000|       [2000]|    2000.0|\n",
      "|   Jeff|Marketing|  3000|       [3000]|    3000.0|\n",
      "|  James|    Sales|  3000| [3000, 3000]|    3000.0|\n",
      "|  James|    Sales|  3000| [3000, 3000]|    3000.0|\n",
      "| Robert|    Sales|  4100| [4100, 4100]|    4100.0|\n",
      "|   Saif|    Sales|  4100| [4100, 4100]|    4100.0|\n",
      "|Michael|    Sales|  4600|       [4600]|    4600.0|\n",
      "+-------+---------+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary').rangeBetween(-400, 400)\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------------------+----------+------------------+----+----+---------+-----+----------+------------+----+\n",
      "|emp_id |dept_id  |salary|salaries_list                 |avg_salary|cume_dist         |lag |lead|nth_value|ntile|dense_rank|percent_rank|rank|\n",
      "+-------+---------+------+------------------------------+----------+------------------+----+----+---------+-----+----------+------------+----+\n",
      "|Maria  |Finance  |3000  |[3000]                        |3000.0    |0.3333333333333333|null|3300|null     |1    |1         |0.0         |1   |\n",
      "|Scott  |Finance  |3300  |[3000, 3300]                  |3150.0    |0.6666666666666666|3000|3900|3300     |1    |2         |0.5         |2   |\n",
      "|Jen    |Finance  |3900  |[3000, 3300, 3900]            |3400.0    |1.0               |3300|null|3300     |2    |3         |1.0         |3   |\n",
      "|Kumar  |Marketing|2000  |[2000]                        |2000.0    |0.5               |null|3000|null     |1    |1         |0.0         |1   |\n",
      "|Jeff   |Marketing|3000  |[2000, 3000]                  |2500.0    |1.0               |2000|null|3000     |2    |2         |1.0         |2   |\n",
      "|James  |Sales    |3000  |[3000, 3000]                  |3000.0    |0.4               |null|3000|3000     |1    |1         |0.0         |1   |\n",
      "|James  |Sales    |3000  |[3000, 3000]                  |3000.0    |0.4               |3000|4100|3000     |1    |1         |0.0         |1   |\n",
      "|Robert |Sales    |4100  |[3000, 3000, 4100, 4100]      |3550.0    |0.8               |3000|4100|3000     |1    |2         |0.5         |3   |\n",
      "|Saif   |Sales    |4100  |[3000, 3000, 4100, 4100]      |3550.0    |0.8               |4100|4600|3000     |2    |2         |0.5         |3   |\n",
      "|Michael|Sales    |4600  |[3000, 3000, 4100, 4100, 4600]|3760.0    |1.0               |4100|null|3000     |2    |3         |1.0         |5   |\n",
      "+-------+---------+------+------------------------------+----------+------------------+----+----+---------+-----+----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary')\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    "        .withColumn('cume_dist', F.cume_dist().over(wspec))\n",
    "        .withColumn('lag', F.lag(df.salary, 1).over(wspec))\n",
    "        .withColumn('lead', F.lead(df.salary, 1).over(wspec))\n",
    "        .withColumn('nth_value', F.nth_value(df.salary, 2).over(wspec))\n",
    "        .withColumn('ntile', F.ntile(2).over(wspec))\n",
    "        .withColumn('dense_rank', F.dense_rank().over(wspec))\n",
    "        .withColumn('percent_rank', F.percent_rank().over(wspec))\n",
    "        .withColumn('rank', F.rank().over(wspec))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 86:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|add_one(rating)|\n",
      "+---------------+\n",
      "|            6.0|\n",
      "|            4.0|\n",
      "|            4.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            4.0|\n",
      "|            6.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            4.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@F.pandas_udf('double', F.PandasUDFType.SCALAR)\n",
    "def add_one(v) -> pd.DataFrame:\n",
    "    return v + 1\n",
    "\n",
    "ratings_df.select(add_one(ratings_df.rating)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 89:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+---------+\n",
      "|user_id|movie_id|      rating|timestamp|\n",
      "+-------+--------+------------+---------+\n",
      "|    181|      31|   0.8865249|977087101|\n",
      "|    195|      31|-0.113475084|991013952|\n",
      "|    203|      31|   -2.113475|976929358|\n",
      "|    223|      31|-0.113475084|976905652|\n",
      "|    268|      31|   0.8865249|976647137|\n",
      "|    368|      31|-0.113475084|976670975|\n",
      "|    517|      31|   0.8865249|976204301|\n",
      "|    524|      31|   -2.113475|976171096|\n",
      "|    528|      31|   1.8865249|980039160|\n",
      "|    531|      31|  -1.1134751|978973034|\n",
      "|    536|      31|-0.113475084|976137228|\n",
      "|    543|      31|   0.8865249|976159357|\n",
      "|    616|      31|-0.113475084|975802599|\n",
      "|    676|      31|   0.8865249|975684957|\n",
      "|    678|      31|   0.8865249|989241973|\n",
      "|    692|      31|-0.113475084|978375055|\n",
      "|    699|      31|-0.113475084|975563262|\n",
      "|    710|      31|-0.113475084|978586309|\n",
      "|    752|      31|   -2.113475|975461295|\n",
      "|    777|      31|-0.113475084|975520841|\n",
      "+-------+--------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@F.pandas_udf(ratings_df.schema, F.PandasUDFType.GROUPED_MAP)\n",
    "# Input/output are both a pandas.DataFrame\n",
    "def subtract_mean(pdf):\n",
    "    return pdf.assign(rating=pdf.rating - pdf.rating.mean())\n",
    "\n",
    "ratings_df.groupby('movie_id').apply(subtract_mean).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
