{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 04: Детали SQL и Spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Находнов Максим (nakhodnov17@gmail.com)`\n",
    "#### `Москва, 2023`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* RDD API\n",
    "* Pivot/Unpivot\n",
    "* Window function\n",
    "* UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для эффективной работы UDF необходимо установить библиотеку, реализующую передачу данных между Spark и Python в Arrow формате:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:02.362387Z",
     "start_time": "2023-02-05T17:46:59.647723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/site-packages (from pyarrow) (1.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:13.832913Z",
     "start_time": "2023-02-05T17:47:04.417093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 20:47:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .setMaster('local[*]')\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfn4tqdooajR"
   },
   "source": [
    "### `Spark RDD API`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6XuSx2Eo16N"
   },
   "source": [
    "* [Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "* [Документация](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:14.929849Z",
     "start_time": "2023-02-05T17:47:14.670825Z"
    },
    "id": "oFdQjWbfouNJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 10),\n",
    "    (2, 41),\n",
    "    (0, 12),\n",
    "    (2, 64),\n",
    "    (2, 22),\n",
    "    (1, 11),\n",
    "    (0, 94),\n",
    "]\n",
    "dist_data = sc.parallelize(data)\n",
    "dist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:16.109485Z",
     "start_time": "2023-02-05T17:47:15.432580Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWDkHLUbpRrA",
    "outputId": "186df13f-6485-4e3b-8b6a-c53fd23673a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                        (0 + 12) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 10), (2, 41), (0, 12), (2, 64), (2, 22), (1, 11), (0, 94)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:17.631983Z",
     "start_time": "2023-02-05T17:47:16.794842Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8lXGP8NpTOM",
    "outputId": "f2e5c344-9986-4955-c5bf-0c64b3f0bc95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                        (0 + 12) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11, 1), (43, 2), (12, 0), (66, 2), (24, 2), (12, 1), (94, 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.map(lambda x: (x[0] + x[1], x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:23.064113Z",
     "start_time": "2023-02-05T17:47:18.261513Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NivHzNFppYVs",
    "outputId": "c79c44b6-c649-436f-8931-9e7812eda325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[key: bigint, value: bigint],\n",
       " [Row(key=1, value=10),\n",
       "  Row(key=2, value=41),\n",
       "  Row(key=0, value=12),\n",
       "  Row(key=2, value=64),\n",
       "  Row(key=2, value=22),\n",
       "  Row(key=1, value=11),\n",
       "  Row(key=0, value=94)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df = dist_data.toDF(['key', 'value'])\n",
    "dist_df, dist_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:24.589599Z",
     "start_time": "2023-02-05T17:47:23.879654Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LeiWR9VptZU",
    "outputId": "1e9f535f-7097-4cfd-8c51-fd445a849039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 106), (1, 21)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.groupByKey().mapValues(sum).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:25.224632Z",
     "start_time": "2023-02-05T17:47:24.592327Z"
    },
    "id": "oNixwVHXrkfy"
   },
   "outputs": [],
   "source": [
    "! echo \"Hello, sample RDD\" > text.txt\n",
    "! echo \"This RDD contains three lines\" >> text.txt\n",
    "! echo \"This is the last line\" >> text.txt\n",
    "! echo \"\" >> text.txt\n",
    "! echo \"Just kidding, it contains five lines\" >> text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:25.903614Z",
     "start_time": "2023-02-05T17:47:25.648415Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXaOLQ7ksEnO",
    "outputId": "4f5d3461-2625-406a-e550-42198a9c029f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(text.txt MapPartitionsRDD[17] at textFile at DirectMethodHandleAccessor.java:104,\n",
       " ['Hello, sample RDD',\n",
       "  'This RDD contains three lines',\n",
       "  'This is the last line',\n",
       "  '',\n",
       "  'Just kidding, it contains five lines'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = sc.textFile('text.txt')\n",
    "text_data, text_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:26.897676Z",
     "start_time": "2023-02-05T17:47:26.451644Z"
    },
    "id": "PoNCwEfFsHvF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[25] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[21] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[20] at partitionBy at DirectMethodHandleAccessor.java:104 []\n",
      " +-(2) PairwiseRDD[19] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2223031120.py:5 []\n",
      "    |  PythonRDD[18] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2223031120.py:5 []\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at DirectMethodHandleAccessor.java:104 []\n",
      "    |  text.txt HadoopRDD[16] at textFile at DirectMethodHandleAccessor.java:104 []\n"
     ]
    }
   ],
   "source": [
    "distinct_words = (\n",
    "    text_data\n",
    "        .filter(lambda x: len(x))\n",
    "        .flatMap(lambda x: x.split(' '))\n",
    "        .distinct()\n",
    ")\n",
    "distinct_words.saveAsTextFile('words.txt')\n",
    "print(distinct_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы переиспользовать посчитанные значения в рамках текущей сессии стоит использовать метод `.cache`, который сохраняет результат вычислений в данной вершине в оперативной памяти.\n",
    "\n",
    "Метод `.persist` позволяет сохранять промежуточные вычисления в рамках текущей сессии с более тонкой настройкой места хранения (жёсткий диск, оперативная память, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:27.926226Z",
     "start_time": "2023-02-05T17:47:27.915019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[25] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |  MapPartitionsRDD[21] at mapPartitions at PythonRDD.scala:145 [Memory Serialized 1x Replicated]\n",
      " |  ShuffledRDD[20] at partitionBy at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      " +-(2) PairwiseRDD[19] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2223031120.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  PythonRDD[18] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2223031120.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt HadoopRDD[16] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "distinct_words_cached = distinct_words.cache()\n",
    "print(distinct_words_cached.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:29.725557Z",
     "start_time": "2023-02-05T17:47:29.600868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[25] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |       CachedPartitions: 2; MemorySize: 298.0 B; DiskSize: 0.0 B\n",
      " |  MapPartitionsRDD[21] at mapPartitions at PythonRDD.scala:145 [Memory Serialized 1x Replicated]\n",
      " |  ShuffledRDD[20] at partitionBy at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      " +-(2) PairwiseRDD[19] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2223031120.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  PythonRDD[18] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2223031120.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt HadoopRDD[16] at textFile at DirectMethodHandleAccessor.java:104 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "distinct_words_cached.collect()\n",
    "print(distinct_words_cached.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для со данных между сессиями сессиями можно использовать `.checkpoint`. Особенность этого метода — изменение графа вычислений. Цепочка вычислений для сохраняемого RDD будет удалена. Сокращение цепочки вычислений полезно в случае больших графов, например, в итеративных алгоритмах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:32.943109Z",
     "start_time": "2023-02-05T17:47:32.903993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[30] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[29] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[28] at partitionBy at DirectMethodHandleAccessor.java:104 []\n",
      " +-(2) PairwiseRDD[27] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2415635507.py:5 []\n",
      "    |  PythonRDD[26] at distinct at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/2415635507.py:5 []\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at DirectMethodHandleAccessor.java:104 []\n",
      "    |  text.txt HadoopRDD[16] at textFile at DirectMethodHandleAccessor.java:104 []\n"
     ]
    }
   ],
   "source": [
    "distinct_first_words = (\n",
    "    text_data\n",
    "        .filter(lambda x: len(x))\n",
    "        .flatMap(lambda x: x.split(' ')[0])\n",
    "        .distinct()\n",
    ")\n",
    "\n",
    "sc.setCheckpointDir('./checkpoints')\n",
    "\n",
    "distinct_first_words.checkpoint()\n",
    "print(distinct_first_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:35.168178Z",
     "start_time": "2023-02-05T17:47:34.634503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[30] at RDD at PythonRDD.scala:53 []\n",
      " |  ReliableCheckpointRDD[31] at collect at /var/folders/3c/vr2463p11lz5fr_80mrg72gh0000gq/T/ipykernel_37355/656455338.py:1 []\n"
     ]
    }
   ],
   "source": [
    "distinct_first_words.collect()\n",
    "print(distinct_first_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pivot/Unpivot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:41.797212Z",
     "start_time": "2023-02-05T17:47:38.864468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-05 20:47:39--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Распознаётся files.grouplens.org (files.grouplens.org)… 128.101.65.152\n",
      "Подключение к files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 5917549 (5,6M) [application/zip]\n",
      "Сохранение в: «ml-1m.zip.1»\n",
      "\n",
      "ml-1m.zip.1         100%[===================>]   5,64M  3,37MB/s    за 1,7s    \n",
      "\n",
      "2023-02-05 20:47:41 (3,37 MB/s) - «ml-1m.zip.1» сохранён [5917549/5917549]\n",
      "\n",
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat        \n",
      "  inflating: ml-1m/ratings.dat       \n",
      "  inflating: ml-1m/README            \n",
      "  inflating: ml-1m/users.dat         \n"
     ]
    }
   ],
   "source": [
    "! wget https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "! unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:42.847648Z",
     "start_time": "2023-02-05T17:47:42.637978Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('movie_id', T.IntegerType())\n",
    "        .add('movie', T.StringType())\n",
    "        .add('categories', T.StringType())\n",
    ")\n",
    "movies_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/movies.dat')\n",
    ")\n",
    "\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('user_id', T.IntegerType())\n",
    "        .add('movie_id', T.IntegerType())\n",
    "        .add('rating', T.FloatType())\n",
    "        .add('timestamp', T.StringType())\n",
    ")\n",
    "ratings_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/ratings.dat')\n",
    ")\n",
    "\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('user_id', T.IntegerType())\n",
    "        .add('gender', T.StringType())\n",
    "        .add('age', T.IntegerType())\n",
    "        .add('occupation', T.IntegerType())\n",
    "        .add('zip-code', T.StringType())\n",
    ")\n",
    "users_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/users.dat')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+-------+--------+------+\n",
    "|user_id|movie_id|rating|\n",
    "+-------+--------+------+\n",
    "|      1|    1193|   5.0|\n",
    "|      1|    1193|   4.0|\n",
    "|      1|     661|   3.0|\n",
    "|      2|     661|   3.0|\n",
    "|      3|    1193|   4.0|\n",
    "|      3|    2355|   5.0|\n",
    "+-------+--------+------+\n",
    "\n",
    "groupBy(user_id).pivot(movie_id).agg(mean(rating))\n",
    "groupBy(user_id).pivot(movie_id).agg(first(rating))\n",
    "\n",
    "1 -> (1193, 5.0), (661, 3.0)\n",
    "2 -> (661, 3.0)\n",
    "3 -> (1193, 4.0), (2355, 5.0)\n",
    "\n",
    "   1193 661 2355\n",
    "1  4.5  3.0 None\n",
    "2  None 3.0 None\n",
    "3  4.0 None 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:44.767745Z",
     "start_time": "2023-02-05T17:47:44.338940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|      1|    1193|   5.0|978300760|\n",
      "|      1|     661|   3.0|978302109|\n",
      "|      1|     914|   3.0|978301968|\n",
      "|      1|    3408|   4.0|978300275|\n",
      "|      1|    2355|   5.0|978824291|\n",
      "|      1|    1197|   3.0|978302268|\n",
      "|      1|    1287|   5.0|978302039|\n",
      "|      1|    2804|   5.0|978300719|\n",
      "|      1|     594|   4.0|978302268|\n",
      "|      1|     919|   4.0|978301368|\n",
      "|      1|     595|   5.0|978824268|\n",
      "|      1|     938|   4.0|978301752|\n",
      "|      1|    2398|   4.0|978302281|\n",
      "|      1|    2918|   4.0|978302124|\n",
      "|      1|    1035|   5.0|978301753|\n",
      "|      1|    2791|   4.0|978302188|\n",
      "|      1|    2687|   3.0|978824268|\n",
      "|      1|    2018|   4.0|978301777|\n",
      "|      1|    3105|   5.0|978301713|\n",
      "|      1|    2797|   4.0|978302039|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:55.452913Z",
     "start_time": "2023-02-05T17:47:45.148611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 20:47:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 20:47:50 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3943</th>\n",
       "      <th>3944</th>\n",
       "      <th>3945</th>\n",
       "      <th>3946</th>\n",
       "      <th>3947</th>\n",
       "      <th>3948</th>\n",
       "      <th>3949</th>\n",
       "      <th>3950</th>\n",
       "      <th>3951</th>\n",
       "      <th>3952</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 3707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    1    2   3   4   5   6    7   8   9  ...  3943  3944  3945  \\\n",
       "0      148  5.0  5.0 NaN NaN NaN NaN  3.0 NaN NaN  ...   NaN   NaN   NaN   \n",
       "\n",
       "   3946  3947  3948  3949  3950  3951  3952  \n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[1 rows x 3707 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df = (\n",
    "    ratings_df\n",
    "        .groupBy(ratings_df.user_id)\n",
    "        .pivot('movie_id')\n",
    "        .agg(F.first(ratings_df.rating))\n",
    ")\n",
    "pivot_df.where(pivot_df.user_id == 148).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:56.214419Z",
     "start_time": "2023-02-05T17:47:55.455273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------------+\n",
      "|movie_id|rates|        avg_rating|\n",
      "+--------+-----+------------------+\n",
      "|    2858| 3428|4.3173862310385065|\n",
      "|     260| 2991| 4.453694416583082|\n",
      "|    1196| 2990| 4.292976588628763|\n",
      "|    1210| 2883| 4.022892819979188|\n",
      "|     480| 2672|3.7638473053892216|\n",
      "|    2028| 2653| 4.337353938937053|\n",
      "|     589| 2649| 4.058512646281616|\n",
      "|    2571| 2590| 4.315830115830116|\n",
      "|    1270| 2583|3.9903213317847466|\n",
      "|     593| 2578|4.3518231186966645|\n",
      "|    1580| 2538| 3.739952718676123|\n",
      "|    1198| 2514| 4.477724741447892|\n",
      "|     608| 2513| 4.254675686430561|\n",
      "|    2762| 2459| 4.406262708418057|\n",
      "|     110| 2443| 4.234957020057307|\n",
      "|    2396| 2369| 4.127479949345715|\n",
      "|    1197| 2318|4.3037100949094045|\n",
      "|     527| 2304| 4.510416666666667|\n",
      "|    1617| 2288| 4.219405594405594|\n",
      "|    1265| 2278| 3.953028972783143|\n",
      "+--------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_movies_df = ratings_df.groupBy(ratings_df.movie_id).agg(\n",
    "    F.count(ratings_df.rating).alias('rates'),\n",
    "    F.mean(ratings_df.rating).alias('avg_rating')\n",
    ").sort('rates', ascending=False).limit(100)\n",
    "\n",
    "top_movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:56.755558Z",
     "start_time": "2023-02-05T17:47:56.217120Z"
    }
   },
   "outputs": [],
   "source": [
    "top_movies = top_movies_df.rdd.map(lambda x: x.movie_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:57.536251Z",
     "start_time": "2023-02-05T17:47:56.759503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>2858</th>\n",
       "      <th>260</th>\n",
       "      <th>1196</th>\n",
       "      <th>1210</th>\n",
       "      <th>480</th>\n",
       "      <th>2028</th>\n",
       "      <th>589</th>\n",
       "      <th>2571</th>\n",
       "      <th>1270</th>\n",
       "      <th>...</th>\n",
       "      <th>2699</th>\n",
       "      <th>750</th>\n",
       "      <th>39</th>\n",
       "      <th>21</th>\n",
       "      <th>1393</th>\n",
       "      <th>2804</th>\n",
       "      <th>588</th>\n",
       "      <th>2406</th>\n",
       "      <th>1220</th>\n",
       "      <th>733</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  2858  260  1196  1210  480  2028  589  2571  1270  ...  2699  750  \\\n",
       "0        1   NaN  4.0   NaN   NaN  NaN   5.0  NaN   NaN   5.0  ...   NaN  NaN   \n",
       "1        2   4.0  NaN   5.0   4.0  5.0   4.0  4.0   4.0   NaN  ...   NaN  NaN   \n",
       "\n",
       "   39   21  1393  2804  588  2406  1220  733  \n",
       "0 NaN  NaN   NaN   5.0  4.0   NaN   NaN  NaN  \n",
       "1 NaN  1.0   NaN   NaN  NaN   NaN   NaN  NaN  \n",
       "\n",
       "[2 rows x 101 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_top_df = (\n",
    "    ratings_df\n",
    "        .groupBy(ratings_df.user_id)\n",
    "        .pivot('movie_id', top_movies)\n",
    "        .agg(F.first(ratings_df.rating))\n",
    ")\n",
    "pivot_top_df.where(pivot_df.user_id < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:58.712056Z",
     "start_time": "2023-02-05T17:47:57.539033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>2858</th>\n",
       "      <th>260</th>\n",
       "      <th>1196</th>\n",
       "      <th>1210</th>\n",
       "      <th>480</th>\n",
       "      <th>2028</th>\n",
       "      <th>589</th>\n",
       "      <th>2571</th>\n",
       "      <th>1270</th>\n",
       "      <th>...</th>\n",
       "      <th>2699</th>\n",
       "      <th>750</th>\n",
       "      <th>39</th>\n",
       "      <th>21</th>\n",
       "      <th>1393</th>\n",
       "      <th>2804</th>\n",
       "      <th>588</th>\n",
       "      <th>2406</th>\n",
       "      <th>1220</th>\n",
       "      <th>733</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  2858  260  1196  1210  480  2028  589  2571  1270  ...  2699  750  \\\n",
       "0        1   3.0  4.0   3.0   3.0  3.0   5.0  3.0   3.0   5.0  ...   3.0  3.0   \n",
       "1        2   4.0  3.0   5.0   4.0  5.0   4.0  4.0   4.0   3.0  ...   3.0  3.0   \n",
       "\n",
       "    39   21  1393  2804  588  2406  1220  733  \n",
       "0  3.0  3.0   3.0   5.0  4.0   3.0   3.0  3.0  \n",
       "1  3.0  1.0   3.0   3.0  3.0   3.0   3.0  3.0  \n",
       "\n",
       "[2 rows x 101 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_top_df = (\n",
    "    ratings_df\n",
    "        .groupBy(ratings_df.user_id)\n",
    "        .pivot('movie_id', top_movies)\n",
    "        .agg(F.first(ratings_df.rating))\n",
    "        .fillna(3.0)\n",
    ")\n",
    "pivot_top_df.where(pivot_top_df.user_id < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:03.087299Z",
     "start_time": "2023-02-05T17:47:58.714144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "5  HOBBIES_1_006_CA_1_validation  HOBBIES_1_006  HOBBIES_1  HOBBIES     CA_1   \n",
       "6  HOBBIES_1_007_CA_1_validation  HOBBIES_1_007  HOBBIES_1  HOBBIES     CA_1   \n",
       "7  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "8  HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "9  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "5       CA    0    0    0    0  ...       0       1       0       1       0   \n",
       "6       CA    0    0    0    0  ...       0       0       0       1       0   \n",
       "7       CA   12   15    0    0  ...       0       0       1      37       3   \n",
       "8       CA    2    0    7    3  ...       0       0       1       1       6   \n",
       "9       CA    0    0    1    0  ...       1       0       0       0       0   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "5       0       0       2       0       0  \n",
       "6       1       0       0       1       1  \n",
       "7       4       6       3       2       1  \n",
       "8       0       0       0       0       0  \n",
       "9       0       0       2       0       2  \n",
       "\n",
       "[10 rows x 1919 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./m5-forecasting-accuracy\"\n",
    "\n",
    "df_validation = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"inferSchema\", True)\n",
    "      .option(\"header\", True)\n",
    "      .option(\"sep\", ',')\n",
    "      .load(f\"{path}/sales_train_validation.csv\")\n",
    ")\n",
    "df_validation.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:03.372957Z",
     "start_time": "2023-02-05T17:48:03.089371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---+-----+\n",
      "|id                           |d  |sales|\n",
      "+-----------------------------+---+-----+\n",
      "|HOBBIES_1_001_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_001_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_002_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_002_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_003_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_003_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_004_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_004_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_005_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_005_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_006_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_006_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_007_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_007_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_008_CA_1_validation|d_1|12   |\n",
      "|HOBBIES_1_008_CA_1_validation|d_2|15   |\n",
      "|HOBBIES_1_009_CA_1_validation|d_1|2    |\n",
      "|HOBBIES_1_009_CA_1_validation|d_2|0    |\n",
      "|HOBBIES_1_010_CA_1_validation|d_1|0    |\n",
      "|HOBBIES_1_010_CA_1_validation|d_2|0    |\n",
      "+-----------------------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_expression = \"stack(2, 'd_1', d_1, 'd_2', d_2) as (d, sales)\"\n",
    "unpivot_df = (\n",
    "    df_validation\n",
    "        .select('id', F.expr(unpivot_expression))\n",
    ")\n",
    "\n",
    "unpivot_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:05.528901Z",
     "start_time": "2023-02-05T17:48:03.376408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 61:====>                                                   (1 + 11) / 12]\r",
      "\r",
      "[Stage 61:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---+---+\n",
      "|id                             |d_1|d_2|\n",
      "+-------------------------------+---+---+\n",
      "|FOODS_2_387_CA_1_validation    |0  |0  |\n",
      "|HOBBIES_1_258_CA_1_validation  |0  |0  |\n",
      "|FOODS_2_322_CA_1_validation    |0  |1  |\n",
      "|FOODS_3_352_CA_1_validation    |0  |0  |\n",
      "|FOODS_1_011_CA_1_validation    |2  |1  |\n",
      "|HOBBIES_1_273_CA_1_validation  |1  |0  |\n",
      "|HOBBIES_1_163_CA_1_validation  |0  |0  |\n",
      "|FOODS_2_011_CA_1_validation    |1  |1  |\n",
      "|FOODS_1_101_CA_2_validation    |0  |0  |\n",
      "|HOBBIES_2_044_CA_2_validation  |0  |0  |\n",
      "|FOODS_3_808_CA_1_validation    |22 |18 |\n",
      "|HOUSEHOLD_1_179_CA_2_validation|9  |5  |\n",
      "|HOBBIES_1_236_CA_2_validation  |0  |0  |\n",
      "|HOBBIES_1_299_CA_2_validation  |0  |0  |\n",
      "|FOODS_1_206_CA_2_validation    |3  |0  |\n",
      "|FOODS_3_529_CA_1_validation    |0  |0  |\n",
      "|FOODS_3_644_CA_1_validation    |3  |1  |\n",
      "|FOODS_3_693_CA_1_validation    |0  |0  |\n",
      "|FOODS_1_054_CA_2_validation    |4  |6  |\n",
      "|HOUSEHOLD_1_157_CA_2_validation|0  |0  |\n",
      "+-------------------------------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    unpivot_df\n",
    "        .groupBy(unpivot_df.id)\n",
    "        .pivot('d')\n",
    "        .agg(F.sum(unpivot_df.sales))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:06.073317Z",
     "start_time": "2023-02-05T17:48:05.531766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---+---+\n",
      "|id                         |d_1|d_2|\n",
      "+---------------------------+---+---+\n",
      "|FOODS_3_808_CA_1_validation|22 |18 |\n",
      "+---------------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_validation\n",
    "        .where(df_validation.id == 'FOODS_3_808_CA_1_validation')\n",
    "        .select('id', 'd_1', 'd_2')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Window function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:16.350717Z",
     "start_time": "2023-02-05T17:48:16.347907Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:17.196801Z",
     "start_time": "2023-02-05T17:48:16.872989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------+---------+------+\n",
      "|emp_id |dept_id  |salary|\n",
      "+-------+---------+------+\n",
      "|James  |Sales    |3000  |\n",
      "|Michael|Sales    |4600  |\n",
      "|Robert |Sales    |4100  |\n",
      "|Maria  |Finance  |3000  |\n",
      "|Scott  |Finance  |3300  |\n",
      "|Jen    |Finance  |3900  |\n",
      "|Jeff   |Marketing|3000  |\n",
      "|Kumar  |Marketing|2000  |\n",
      "|Saif   |Sales    |4100  |\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Michael', 'Sales', 4600),\n",
    "    ('Robert', 'Sales', 4100),\n",
    "    ('Maria', 'Finance', 3000),\n",
    "    ('Scott', 'Finance', 3300),\n",
    "    ('Jen', 'Finance', 3900), \n",
    "    ('Jeff', 'Marketing', 3000),\n",
    "    ('Kumar', 'Marketing', 2000),\n",
    "    ('Saif', 'Sales', 4100)\n",
    "]\n",
    " \n",
    "columns= [\"emp_id\", \"dept_id\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema(), df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:18.373245Z",
     "start_time": "2023-02-05T17:48:17.740127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|  dept_id| emp_id|salary|avg_salary|\n",
      "+---------+-------+------+----------+\n",
      "|    Sales|  James|  3000|    3950.0|\n",
      "|    Sales|Michael|  4600|    3950.0|\n",
      "|    Sales| Robert|  4100|    3950.0|\n",
      "|  Finance|  Maria|  3000|    3400.0|\n",
      "|  Finance|  Scott|  3300|    3400.0|\n",
      "|  Finance|    Jen|  3900|    3400.0|\n",
      "|Marketing|   Jeff|  3000|    2500.0|\n",
      "|Marketing|  Kumar|  2000|    2500.0|\n",
      "|    Sales|   Saif|  4100|    3950.0|\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_salaries_df = df.groupBy(df.dept_id).agg(F.mean(df.salary).alias('avg_salary'))\n",
    "df.join(avg_salaries_df, on='dept_id', how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:18.796575Z",
     "start_time": "2023-02-05T17:48:18.376876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------------+----------+\n",
      "|emp_id |dept_id  |salary|salaries_list           |avg_salary|\n",
      "+-------+---------+------+------------------------+----------+\n",
      "|Maria  |Finance  |3000  |[3000, 3300, 3900]      |3400.0    |\n",
      "|Scott  |Finance  |3300  |[3000, 3300, 3900]      |3400.0    |\n",
      "|Jen    |Finance  |3900  |[3000, 3300, 3900]      |3400.0    |\n",
      "|Jeff   |Marketing|3000  |[3000, 2000]            |2500.0    |\n",
      "|Kumar  |Marketing|2000  |[3000, 2000]            |2500.0    |\n",
      "|James  |Sales    |3000  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "|Michael|Sales    |4600  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "|Robert |Sales    |4100  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "|Saif   |Sales    |4100  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "+-------+---------+------+------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id')\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:19.222300Z",
     "start_time": "2023-02-05T17:48:18.799787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+----------+------------------+\n",
      "| emp_id|  dept_id|salary|       salaries_list|row_number|        avg_salary|\n",
      "+-------+---------+------+--------------------+----------+------------------+\n",
      "|  Maria|  Finance|  3000|              [3000]|         1|            3000.0|\n",
      "|  Scott|  Finance|  3300|        [3000, 3300]|         2|            3150.0|\n",
      "|    Jen|  Finance|  3900|  [3000, 3300, 3900]|         3|            3400.0|\n",
      "|  Kumar|Marketing|  2000|              [2000]|         1|            2000.0|\n",
      "|   Jeff|Marketing|  3000|        [2000, 3000]|         2|            2500.0|\n",
      "|  James|    Sales|  3000|              [3000]|         1|            3000.0|\n",
      "| Robert|    Sales|  4100|  [3000, 4100, 4100]|         2|3733.3333333333335|\n",
      "|   Saif|    Sales|  4100|  [3000, 4100, 4100]|         3|3733.3333333333335|\n",
      "|Michael|    Sales|  4600|[3000, 4100, 4100...|         4|            3950.0|\n",
      "+-------+---------+------+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary')\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('row_number', F.row_number().over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:19.547708Z",
     "start_time": "2023-02-05T17:48:19.224927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+-----------------+\n",
      "| emp_id|  dept_id|salary|       salaries_list|       avg_salary|\n",
      "+-------+---------+------+--------------------+-----------------+\n",
      "|  Maria|  Finance|  3000|  [3000, 3300, 3900]|           3400.0|\n",
      "|  Scott|  Finance|  3300|        [3300, 3900]|           3600.0|\n",
      "|    Jen|  Finance|  3900|              [3900]|           3900.0|\n",
      "|  Kumar|Marketing|  2000|        [2000, 3000]|           2500.0|\n",
      "|   Jeff|Marketing|  3000|              [3000]|           3000.0|\n",
      "|  James|    Sales|  3000|[3000, 4100, 4100...|           3950.0|\n",
      "| Robert|    Sales|  4100|  [4100, 4100, 4600]|4266.666666666667|\n",
      "|   Saif|    Sales|  4100|        [4100, 4600]|           4350.0|\n",
      "|Michael|    Sales|  4600|              [4600]|           4600.0|\n",
      "+-------+---------+------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = (\n",
    "    Window\n",
    "        .partitionBy('dept_id')\n",
    "        .orderBy('salary')\n",
    "        .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    ")\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:19.857532Z",
     "start_time": "2023-02-05T17:48:19.550972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-------------+----------+\n",
      "| emp_id|  dept_id|salary|salaries_list|avg_salary|\n",
      "+-------+---------+------+-------------+----------+\n",
      "|  Maria|  Finance|  3000| [3000, 3300]|    3150.0|\n",
      "|  Scott|  Finance|  3300| [3000, 3300]|    3150.0|\n",
      "|    Jen|  Finance|  3900|       [3900]|    3900.0|\n",
      "|  Kumar|Marketing|  2000|       [2000]|    2000.0|\n",
      "|   Jeff|Marketing|  3000|       [3000]|    3000.0|\n",
      "|  James|    Sales|  3000|       [3000]|    3000.0|\n",
      "| Robert|    Sales|  4100| [4100, 4100]|    4100.0|\n",
      "|   Saif|    Sales|  4100| [4100, 4100]|    4100.0|\n",
      "|Michael|    Sales|  4600|       [4600]|    4600.0|\n",
      "+-------+---------+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary').rangeBetween(-400, 400)\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:20.312702Z",
     "start_time": "2023-02-05T17:48:19.930349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------------+------------------+------------------+----+----+\n",
      "|emp_id |dept_id  |salary|salaries_list           |avg_salary        |cume_dist         |lag |lead|\n",
      "+-------+---------+------+------------------------+------------------+------------------+----+----+\n",
      "|Maria  |Finance  |3000  |[3000]                  |3000.0            |0.3333333333333333|null|3300|\n",
      "|Scott  |Finance  |3300  |[3000, 3300]            |3150.0            |0.6666666666666666|3000|3900|\n",
      "|Jen    |Finance  |3900  |[3000, 3300, 3900]      |3400.0            |1.0               |3300|null|\n",
      "|Kumar  |Marketing|2000  |[2000]                  |2000.0            |0.5               |null|3000|\n",
      "|Jeff   |Marketing|3000  |[2000, 3000]            |2500.0            |1.0               |2000|null|\n",
      "|James  |Sales    |3000  |[3000]                  |3000.0            |0.25              |null|4100|\n",
      "|Robert |Sales    |4100  |[3000, 4100, 4100]      |3733.3333333333335|0.75              |3000|4100|\n",
      "|Saif   |Sales    |4100  |[3000, 4100, 4100]      |3733.3333333333335|0.75              |4100|4600|\n",
      "|Michael|Sales    |4600  |[3000, 4100, 4100, 4600]|3950.0            |1.0               |4100|null|\n",
      "+-------+---------+------+------------------------+------------------+------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary')\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    "        .withColumn('cume_dist', F.cume_dist().over(wspec))\n",
    "        .withColumn('lag', F.lag(df.salary, 1).over(wspec))\n",
    "        .withColumn('lead', F.lead(df.salary, 1).over(wspec))\n",
    ").show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:20.909819Z",
     "start_time": "2023-02-05T17:48:20.481904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------------+---------+-----+----------+------------------+----+\n",
      "|emp_id |dept_id  |salary|salaries_list           |nth_value|ntile|dense_rank|percent_rank      |rank|\n",
      "+-------+---------+------+------------------------+---------+-----+----------+------------------+----+\n",
      "|Maria  |Finance  |3000  |[3000]                  |null     |1    |1         |0.0               |1   |\n",
      "|Scott  |Finance  |3300  |[3000, 3300]            |3300     |1    |2         |0.5               |2   |\n",
      "|Jen    |Finance  |3900  |[3000, 3300, 3900]      |3300     |2    |3         |1.0               |3   |\n",
      "|Kumar  |Marketing|2000  |[2000]                  |null     |1    |1         |0.0               |1   |\n",
      "|Jeff   |Marketing|3000  |[2000, 3000]            |3000     |2    |2         |1.0               |2   |\n",
      "|James  |Sales    |3000  |[3000]                  |null     |1    |1         |0.0               |1   |\n",
      "|Robert |Sales    |4100  |[3000, 4100, 4100]      |4100     |1    |2         |0.3333333333333333|2   |\n",
      "|Saif   |Sales    |4100  |[3000, 4100, 4100]      |4100     |2    |2         |0.3333333333333333|2   |\n",
      "|Michael|Sales    |4600  |[3000, 4100, 4100, 4600]|4100     |2    |3         |1.0               |4   |\n",
      "+-------+---------+------+------------------------+---------+-----+----------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df      \n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('nth_value', F.nth_value(df.salary, 2).over(wspec))\n",
    "        .withColumn('ntile', F.ntile(2).over(wspec))\n",
    "        .withColumn('dense_rank', F.dense_rank().over(wspec))\n",
    "        .withColumn('percent_rank', F.percent_rank().over(wspec))\n",
    "        .withColumn('rank', F.rank().over(wspec))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `UDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:24.610784Z",
     "start_time": "2023-02-05T17:48:22.085861Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|add_one(rating)|\n",
      "+---------------+\n",
      "|            6.0|\n",
      "|            4.0|\n",
      "|            4.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            4.0|\n",
      "|            6.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            4.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+--------+\n",
      "|user_id|movie_id|rating|timestamp|plus_one|\n",
      "+-------+--------+------+---------+--------+\n",
      "|      1|    1193|   5.0|978300760|     6.0|\n",
      "|      1|     661|   3.0|978302109|     4.0|\n",
      "|      1|     914|   3.0|978301968|     4.0|\n",
      "|      1|    3408|   4.0|978300275|     5.0|\n",
      "|      1|    2355|   5.0|978824291|     6.0|\n",
      "|      1|    1197|   3.0|978302268|     4.0|\n",
      "|      1|    1287|   5.0|978302039|     6.0|\n",
      "|      1|    2804|   5.0|978300719|     6.0|\n",
      "|      1|     594|   4.0|978302268|     5.0|\n",
      "|      1|     919|   4.0|978301368|     5.0|\n",
      "|      1|     595|   5.0|978824268|     6.0|\n",
      "|      1|     938|   4.0|978301752|     5.0|\n",
      "|      1|    2398|   4.0|978302281|     5.0|\n",
      "|      1|    2918|   4.0|978302124|     5.0|\n",
      "|      1|    1035|   5.0|978301753|     6.0|\n",
      "|      1|    2791|   4.0|978302188|     5.0|\n",
      "|      1|    2687|   3.0|978824268|     4.0|\n",
      "|      1|    2018|   4.0|978301777|     5.0|\n",
      "|      1|    3105|   5.0|978301713|     6.0|\n",
      "|      1|    2797|   4.0|978302039|     5.0|\n",
      "+-------+--------+------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@F.pandas_udf('double', F.PandasUDFType.SCALAR)\n",
    "def add_one(v):\n",
    "    return v + 1\n",
    "\n",
    "ratings_df.select(add_one(ratings_df.rating)).show()\n",
    "ratings_df.withColumn(\n",
    "    'plus_one', add_one(ratings_df.rating)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:26.332484Z",
     "start_time": "2023-02-05T17:48:24.613454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 101:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+---------+\n",
      "|user_id|movie_id|      rating|timestamp|\n",
      "+-------+--------+------------+---------+\n",
      "|    181|      31|   0.8865249|977087101|\n",
      "|    195|      31|-0.113475084|991013952|\n",
      "|    203|      31|   -2.113475|976929358|\n",
      "|    223|      31|-0.113475084|976905652|\n",
      "|    268|      31|   0.8865249|976647137|\n",
      "|    368|      31|-0.113475084|976670975|\n",
      "|    517|      31|   0.8865249|976204301|\n",
      "|    524|      31|   -2.113475|976171096|\n",
      "|    528|      31|   1.8865249|980039160|\n",
      "|    531|      31|  -1.1134751|978973034|\n",
      "|    536|      31|-0.113475084|976137228|\n",
      "|    543|      31|   0.8865249|976159357|\n",
      "|    616|      31|-0.113475084|975802599|\n",
      "|    676|      31|   0.8865249|975684957|\n",
      "|    678|      31|   0.8865249|989241973|\n",
      "|    692|      31|-0.113475084|978375055|\n",
      "|    699|      31|-0.113475084|975563262|\n",
      "|    710|      31|-0.113475084|978586309|\n",
      "|    752|      31|   -2.113475|975461295|\n",
      "|    777|      31|-0.113475084|975520841|\n",
      "+-------+--------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@F.pandas_udf(ratings_df.schema, F.PandasUDFType.GROUPED_MAP)\n",
    "# Input/output are both a pandas.DataFrame\n",
    "def subtract_mean(pdf):\n",
    "    return pdf.assign(rating=pdf.rating - pdf.rating.mean())\n",
    "\n",
    "ratings_df.groupby('movie_id').apply(subtract_mean).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
